<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Reference · Krylov.jl</title><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.039/juliamono-regular.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.11/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><script src="../../copy.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script><link href="../assets/style.css" rel="stylesheet" type="text/css"/></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../"><img src="../assets/logo.png" alt="Krylov.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../">Krylov.jl</a></span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../api/">API</a></li><li><span class="tocitem">Krylov methods</span><ul><li><a class="tocitem" href="../solvers/spd/">Symmetric positive definite linear systems</a></li><li><a class="tocitem" href="../solvers/sid/">Symmetric indefinite linear systems</a></li><li><a class="tocitem" href="../solvers/unsymmetric/">Unsymmetric linear systems</a></li><li><a class="tocitem" href="../solvers/ln/">Least-norm problems</a></li><li><a class="tocitem" href="../solvers/ls/">Least-squares problems</a></li><li><a class="tocitem" href="../solvers/as/">Adjoint systems</a></li><li><a class="tocitem" href="../solvers/sp_sqd/">Saddle-point and symmetric quasi-definite systems</a></li><li><a class="tocitem" href="../solvers/gsp/">Generalized saddle-point and unsymmetric partitioned systems</a></li></ul></li><li><a class="tocitem" href="../inplace/">In-place methods</a></li><li><a class="tocitem" href="../gpu/">GPU support</a></li><li><a class="tocitem" href="../warm_start/">Warm start</a></li><li><a class="tocitem" href="../factorization-free/">Factorization-free operators</a></li><li><a class="tocitem" href="../tips/">Performance tips</a></li><li><a class="tocitem" href="../examples/">Tutorial</a></li><li class="is-active"><a class="tocitem" href>Reference</a><ul class="internal"><li><a class="tocitem" href="#Contents"><span>Contents</span></a></li><li><a class="tocitem" href="#Index"><span>Index</span></a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Reference</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Reference</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/JuliaSmoothOptimizers/Krylov.jl/blob/master/docs/src/reference.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Reference"><a class="docs-heading-anchor" href="#Reference">Reference</a><a id="Reference-1"></a><a class="docs-heading-anchor-permalink" href="#Reference" title="Permalink"></a></h1><h2 id="Contents"><a class="docs-heading-anchor" href="#Contents">Contents</a><a id="Contents-1"></a><a class="docs-heading-anchor-permalink" href="#Contents" title="Permalink"></a></h2><ul><li><a href="#Reference">Reference</a></li><li class="no-marker"><ul><li><a href="#Contents">Contents</a></li><li><a href="#Index">Index</a></li></ul></li></ul><h2 id="Index"><a class="docs-heading-anchor" href="#Index">Index</a><a id="Index-1"></a><a class="docs-heading-anchor-permalink" href="#Index" title="Permalink"></a></h2><ul><li><a href="#Base.show-Tuple{IO, KrylovSolver}"><code>Base.show</code></a></li><li><a href="#Krylov.Aprod"><code>Krylov.Aprod</code></a></li><li><a href="#Krylov.Atprod"><code>Krylov.Atprod</code></a></li><li><a href="#Krylov.bicgstab-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T&lt;:AbstractFloat"><code>Krylov.bicgstab</code></a></li><li><a href="#Krylov.bicgstab!-Union{Tuple{S}, Tuple{T}, Tuple{BicgstabSolver{T, S}, Any, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}"><code>Krylov.bicgstab!</code></a></li><li><a href="#Krylov.bilq-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T&lt;:AbstractFloat"><code>Krylov.bilq</code></a></li><li><a href="#Krylov.bilq!-Union{Tuple{S}, Tuple{T}, Tuple{BilqSolver{T, S}, Any, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}"><code>Krylov.bilq!</code></a></li><li><a href="#Krylov.bilqr-Union{Tuple{T}, Tuple{Any, AbstractVector{T}, AbstractVector{T}}} where T&lt;:AbstractFloat"><code>Krylov.bilqr</code></a></li><li><a href="#Krylov.bilqr!-Union{Tuple{S}, Tuple{T}, Tuple{BilqrSolver{T, S}, Any, AbstractVector{T}, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}"><code>Krylov.bilqr!</code></a></li><li><a href="#Krylov.cg-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T&lt;:AbstractFloat"><code>Krylov.cg</code></a></li><li><a href="#Krylov.cg!-Union{Tuple{S}, Tuple{T}, Tuple{CgSolver{T, S}, Any, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}"><code>Krylov.cg!</code></a></li><li><a href="#Krylov.cg_lanczos-Union{Tuple{T}, Tuple{Any, AbstractVector{T}, AbstractVector{T}}} where T&lt;:AbstractFloat"><code>Krylov.cg_lanczos</code></a></li><li><a href="#Krylov.cg_lanczos-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T&lt;:AbstractFloat"><code>Krylov.cg_lanczos</code></a></li><li><a href="#Krylov.cg_lanczos!-Union{Tuple{S}, Tuple{T}, Tuple{CgLanczosShiftSolver{T, S}, Any, AbstractVector{T}, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}"><code>Krylov.cg_lanczos!</code></a></li><li><a href="#Krylov.cg_lanczos!-Union{Tuple{S}, Tuple{T}, Tuple{CgLanczosSolver{T, S}, Any, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}"><code>Krylov.cg_lanczos!</code></a></li><li><a href="#Krylov.cgls-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T&lt;:AbstractFloat"><code>Krylov.cgls</code></a></li><li><a href="#Krylov.cgls!-Union{Tuple{S}, Tuple{T}, Tuple{CglsSolver{T, S}, Any, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}"><code>Krylov.cgls!</code></a></li><li><a href="#Krylov.cgne-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T&lt;:AbstractFloat"><code>Krylov.cgne</code></a></li><li><a href="#Krylov.cgne!-Union{Tuple{S}, Tuple{T}, Tuple{CgneSolver{T, S}, Any, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}"><code>Krylov.cgne!</code></a></li><li><a href="#Krylov.cgs-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T&lt;:AbstractFloat"><code>Krylov.cgs</code></a></li><li><a href="#Krylov.cgs!-Union{Tuple{S}, Tuple{T}, Tuple{CgsSolver{T, S}, Any, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}"><code>Krylov.cgs!</code></a></li><li><a href="#Krylov.cr-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T&lt;:AbstractFloat"><code>Krylov.cr</code></a></li><li><a href="#Krylov.cr!-Union{Tuple{S}, Tuple{T}, Tuple{CrSolver{T, S}, Any, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}"><code>Krylov.cr!</code></a></li><li><a href="#Krylov.craig-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T&lt;:AbstractFloat"><code>Krylov.craig</code></a></li><li><a href="#Krylov.craig!-Union{Tuple{S}, Tuple{T}, Tuple{CraigSolver{T, S}, Any, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}"><code>Krylov.craig!</code></a></li><li><a href="#Krylov.craigmr-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T&lt;:AbstractFloat"><code>Krylov.craigmr</code></a></li><li><a href="#Krylov.craigmr!-Union{Tuple{S}, Tuple{T}, Tuple{CraigmrSolver{T, S}, Any, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}"><code>Krylov.craigmr!</code></a></li><li><a href="#Krylov.crls-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T&lt;:AbstractFloat"><code>Krylov.crls</code></a></li><li><a href="#Krylov.crls!-Union{Tuple{S}, Tuple{T}, Tuple{CrlsSolver{T, S}, Any, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}"><code>Krylov.crls!</code></a></li><li><a href="#Krylov.crmr-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T&lt;:AbstractFloat"><code>Krylov.crmr</code></a></li><li><a href="#Krylov.crmr!-Union{Tuple{S}, Tuple{T}, Tuple{CrmrSolver{T, S}, Any, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}"><code>Krylov.crmr!</code></a></li><li><a href="#Krylov.diom-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T&lt;:AbstractFloat"><code>Krylov.diom</code></a></li><li><a href="#Krylov.diom!-Union{Tuple{S}, Tuple{T}, Tuple{DiomSolver{T, S}, Any, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}"><code>Krylov.diom!</code></a></li><li><a href="#Krylov.dqgmres-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T&lt;:AbstractFloat"><code>Krylov.dqgmres</code></a></li><li><a href="#Krylov.dqgmres!-Union{Tuple{S}, Tuple{T}, Tuple{DqgmresSolver{T, S}, Any, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}"><code>Krylov.dqgmres!</code></a></li><li><a href="#Krylov.fom-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T&lt;:AbstractFloat"><code>Krylov.fom</code></a></li><li><a href="#Krylov.fom!-Union{Tuple{S}, Tuple{T}, Tuple{FomSolver{T, S}, Any, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}"><code>Krylov.fom!</code></a></li><li><a href="#Krylov.gmres-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T&lt;:AbstractFloat"><code>Krylov.gmres</code></a></li><li><a href="#Krylov.gmres!-Union{Tuple{S}, Tuple{T}, Tuple{GmresSolver{T, S}, Any, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}"><code>Krylov.gmres!</code></a></li><li><a href="#Krylov.gpmr-Union{Tuple{T}, Tuple{Any, Any, AbstractVector{T}, AbstractVector{T}}} where T&lt;:AbstractFloat"><code>Krylov.gpmr</code></a></li><li><a href="#Krylov.gpmr!-Union{Tuple{S}, Tuple{T}, Tuple{GpmrSolver{T, S}, Any, Any, AbstractVector{T}, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}"><code>Krylov.gpmr!</code></a></li><li><a href="#Krylov.kones-Tuple{Any, Any}"><code>Krylov.kones</code></a></li><li><a href="#Krylov.ktypeof-Tuple{AbstractVector}"><code>Krylov.ktypeof</code></a></li><li><a href="#Krylov.kzeros-Tuple{Any, Any}"><code>Krylov.kzeros</code></a></li><li><a href="#Krylov.lnlq-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T&lt;:AbstractFloat"><code>Krylov.lnlq</code></a></li><li><a href="#Krylov.lnlq!-Union{Tuple{S}, Tuple{T}, Tuple{LnlqSolver{T, S}, Any, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}"><code>Krylov.lnlq!</code></a></li><li><a href="#Krylov.lslq-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T&lt;:AbstractFloat"><code>Krylov.lslq</code></a></li><li><a href="#Krylov.lslq!-Union{Tuple{S}, Tuple{T}, Tuple{LslqSolver{T, S}, Any, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}"><code>Krylov.lslq!</code></a></li><li><a href="#Krylov.lsmr-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T&lt;:AbstractFloat"><code>Krylov.lsmr</code></a></li><li><a href="#Krylov.lsmr!-Union{Tuple{S}, Tuple{T}, Tuple{LsmrSolver{T, S}, Any, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}"><code>Krylov.lsmr!</code></a></li><li><a href="#Krylov.lsqr-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T&lt;:AbstractFloat"><code>Krylov.lsqr</code></a></li><li><a href="#Krylov.lsqr!-Union{Tuple{S}, Tuple{T}, Tuple{LsqrSolver{T, S}, Any, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}"><code>Krylov.lsqr!</code></a></li><li><a href="#Krylov.minres-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T&lt;:AbstractFloat"><code>Krylov.minres</code></a></li><li><a href="#Krylov.minres!-Union{Tuple{S}, Tuple{T}, Tuple{MinresSolver{T, S}, Any, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}"><code>Krylov.minres!</code></a></li><li><a href="#Krylov.minres_qlp-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T&lt;:AbstractFloat"><code>Krylov.minres_qlp</code></a></li><li><a href="#Krylov.minres_qlp!-Union{Tuple{S}, Tuple{T}, Tuple{MinresQlpSolver{T, S}, Any, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}"><code>Krylov.minres_qlp!</code></a></li><li><a href="#Krylov.niterations"><code>Krylov.niterations</code></a></li><li><a href="#Krylov.qmr-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T&lt;:AbstractFloat"><code>Krylov.qmr</code></a></li><li><a href="#Krylov.qmr!-Union{Tuple{S}, Tuple{T}, Tuple{QmrSolver{T, S}, Any, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}"><code>Krylov.qmr!</code></a></li><li><a href="#Krylov.roots_quadratic-Union{Tuple{T}, Tuple{T, T, T}} where T&lt;:AbstractFloat"><code>Krylov.roots_quadratic</code></a></li><li><a href="#Krylov.sym_givens-Union{Tuple{T}, Tuple{T, T}} where T&lt;:AbstractFloat"><code>Krylov.sym_givens</code></a></li><li><a href="#Krylov.sym_givens-Union{Tuple{T}, Tuple{Complex{T}, Complex{T}}} where T&lt;:AbstractFloat"><code>Krylov.sym_givens</code></a></li><li><a href="#Krylov.symmlq-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T&lt;:AbstractFloat"><code>Krylov.symmlq</code></a></li><li><a href="#Krylov.symmlq!-Union{Tuple{S}, Tuple{T}, Tuple{SymmlqSolver{T, S}, Any, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}"><code>Krylov.symmlq!</code></a></li><li><a href="#Krylov.to_boundary-Union{Tuple{T}, Tuple{Vector{T}, Vector{T}, T}} where T&lt;:Number"><code>Krylov.to_boundary</code></a></li><li><a href="#Krylov.tricg-Union{Tuple{T}, Tuple{Any, AbstractVector{T}, AbstractVector{T}}} where T&lt;:AbstractFloat"><code>Krylov.tricg</code></a></li><li><a href="#Krylov.tricg!-Union{Tuple{S}, Tuple{T}, Tuple{TricgSolver{T, S}, Any, AbstractVector{T}, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}"><code>Krylov.tricg!</code></a></li><li><a href="#Krylov.trilqr-Union{Tuple{T}, Tuple{Any, AbstractVector{T}, AbstractVector{T}}} where T&lt;:AbstractFloat"><code>Krylov.trilqr</code></a></li><li><a href="#Krylov.trilqr!-Union{Tuple{S}, Tuple{T}, Tuple{TrilqrSolver{T, S}, Any, AbstractVector{T}, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}"><code>Krylov.trilqr!</code></a></li><li><a href="#Krylov.trimr-Union{Tuple{T}, Tuple{Any, AbstractVector{T}, AbstractVector{T}}} where T&lt;:AbstractFloat"><code>Krylov.trimr</code></a></li><li><a href="#Krylov.trimr!-Union{Tuple{S}, Tuple{T}, Tuple{TrimrSolver{T, S}, Any, AbstractVector{T}, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}"><code>Krylov.trimr!</code></a></li><li><a href="#Krylov.usymlq-Union{Tuple{T}, Tuple{Any, AbstractVector{T}, AbstractVector{T}}} where T&lt;:AbstractFloat"><code>Krylov.usymlq</code></a></li><li><a href="#Krylov.usymlq!-Union{Tuple{S}, Tuple{T}, Tuple{UsymlqSolver{T, S}, Any, AbstractVector{T}, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}"><code>Krylov.usymlq!</code></a></li><li><a href="#Krylov.usymqr-Union{Tuple{T}, Tuple{Any, AbstractVector{T}, AbstractVector{T}}} where T&lt;:AbstractFloat"><code>Krylov.usymqr</code></a></li><li><a href="#Krylov.usymqr!-Union{Tuple{S}, Tuple{T}, Tuple{UsymqrSolver{T, S}, Any, AbstractVector{T}, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}"><code>Krylov.usymqr!</code></a></li><li><a href="#Krylov.vec2str-Union{Tuple{AbstractVector{T}}, Tuple{T}} where T&lt;:Union{Missing, AbstractFloat}"><code>Krylov.vec2str</code></a></li></ul><article class="docstring"><header><a class="docstring-binding" id="Base.show-Tuple{IO, KrylovSolver}" href="#Base.show-Tuple{IO, KrylovSolver}"><code>Base.show</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">show(io, solver; show_stats=true)</code></pre><p>Statistics of <code>solver</code> are displayed if <code>show_stats</code> is set to true.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/Krylov.jl/blob/94caa506e3363ec846b33c79ccc8889d96055e85/src/krylov_solvers.jl#L1611-L1615">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Krylov.Aprod" href="#Krylov.Aprod"><code>Krylov.Aprod</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">Aprod(solver)</code></pre><p>Return the number of operator-vector products with <code>A</code> performed by the Krylov method associated to <code>solver</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/Krylov.jl/blob/94caa506e3363ec846b33c79ccc8889d96055e85/src/krylov_solvers.jl#L1539-L1543">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Krylov.Atprod" href="#Krylov.Atprod"><code>Krylov.Atprod</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">Atprod(solver)</code></pre><p>Return the number of operator-vector products with <code>A&#39;</code> performed by the Krylov method associated to <code>solver</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/Krylov.jl/blob/94caa506e3363ec846b33c79ccc8889d96055e85/src/krylov_solvers.jl#L1546-L1550">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Krylov.bicgstab!-Union{Tuple{S}, Tuple{T}, Tuple{BicgstabSolver{T, S}, Any, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}" href="#Krylov.bicgstab!-Union{Tuple{S}, Tuple{T}, Tuple{BicgstabSolver{T, S}, Any, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}"><code>Krylov.bicgstab!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">solver = bicgstab!(solver::BicgstabSolver, args...; kwargs...)</code></pre><p>where <code>args</code> and <code>kwargs</code> are arguments and keyword arguments of <a href="../solvers/unsymmetric/#Krylov.bicgstab"><code>bicgstab</code></a>.</p><p>See <a href="../api/#Krylov.BicgstabSolver"><code>BicgstabSolver</code></a> for more details about the <code>solver</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/Krylov.jl/blob/94caa506e3363ec846b33c79ccc8889d96055e85/src/bicgstab.jl#L52-L58">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Krylov.bicgstab-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T&lt;:AbstractFloat" href="#Krylov.bicgstab-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T&lt;:AbstractFloat"><code>Krylov.bicgstab</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">(x, stats) = bicgstab(A, b::AbstractVector{T}; c::AbstractVector{T}=b,
                      M=I, N=I, atol::T=√eps(T), rtol::T=√eps(T),
                      itmax::Int=0, verbose::Int=0, history::Bool=false) where T &lt;: AbstractFloat</code></pre><p>Solve the square linear system Ax = b using the BICGSTAB method. BICGSTAB requires two initial vectors <code>b</code> and <code>c</code>. The relation <code>bᵀc ≠ 0</code> must be satisfied and by default <code>c = b</code>.</p><p>The Biconjugate Gradient Stabilized method is a variant of BiCG, like CGS, but using different updates for the Aᵀ-sequence in order to obtain smoother convergence than CGS.</p><p>If BICGSTAB stagnates, we recommend DQGMRES and BiLQ as alternative methods for unsymmetric square systems.</p><p>BICGSTAB stops when <code>itmax</code> iterations are reached or when <code>‖rₖ‖ ≤ atol + ‖b‖ * rtol</code>. <code>atol</code> is an absolute tolerance and <code>rtol</code> is a relative tolerance.</p><p>Additional details can be displayed if verbose mode is enabled (verbose &gt; 0). Information will be displayed every <code>verbose</code> iterations.</p><p>This implementation allows a left preconditioner <code>M</code> and a right preconditioner <code>N</code>.</p><p><strong>References</strong></p><ul><li>H. A. van der Vorst, <a href="https://doi.org/10.1137/0913035"><em>Bi-CGSTAB: A fast and smoothly converging variant of Bi-CG for the solution of nonsymmetric linear systems</em></a>, SIAM Journal on Scientific and Statistical Computing, 13(2), pp. 631–644, 1992.</li><li>G. L.G. Sleijpen and D. R. Fokkema, <em>BiCGstab(ℓ) for linear equations involving unsymmetric matrices with complex spectrum</em>, Electronic Transactions on Numerical Analysis, 1, pp. 11–32, 1993.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/Krylov.jl/blob/94caa506e3363ec846b33c79ccc8889d96055e85/src/bicgstab.jl#L18-L45">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Krylov.bilq!-Union{Tuple{S}, Tuple{T}, Tuple{BilqSolver{T, S}, Any, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}" href="#Krylov.bilq!-Union{Tuple{S}, Tuple{T}, Tuple{BilqSolver{T, S}, Any, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}"><code>Krylov.bilq!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">solver = bilq!(solver::BilqSolver, args...; kwargs...)</code></pre><p>where <code>args</code> and <code>kwargs</code> are arguments and keyword arguments of <a href="../solvers/unsymmetric/#Krylov.bilq"><code>bilq</code></a>.</p><p>See <a href="../api/#Krylov.BilqSolver"><code>BilqSolver</code></a> for more details about the <code>solver</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/Krylov.jl/blob/94caa506e3363ec846b33c79ccc8889d96055e85/src/bilq.jl#L39-L45">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Krylov.bilq-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T&lt;:AbstractFloat" href="#Krylov.bilq-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T&lt;:AbstractFloat"><code>Krylov.bilq</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">(x, stats) = bilq(A, b::AbstractVector{T}; c::AbstractVector{T}=b,
                  atol::T=√eps(T), rtol::T=√eps(T), transfer_to_bicg::Bool=true,
                  itmax::Int=0, verbose::Int=0, history::Bool=false) where T &lt;: AbstractFloat</code></pre><p>Solve the square linear system Ax = b using the BiLQ method.</p><p>BiLQ is based on the Lanczos biorthogonalization process and requires two initial vectors <code>b</code> and <code>c</code>. The relation <code>bᵀc ≠ 0</code> must be satisfied and by default <code>c = b</code>. When <code>A</code> is symmetric and <code>b = c</code>, BiLQ is equivalent to SYMMLQ.</p><p>An option gives the possibility of transferring to the BiCG point, when it exists. The transfer is based on the residual norm.</p><p><strong>Reference</strong></p><ul><li>A. Montoison and D. Orban, <a href="https://doi.org/10.1137/19M1290991"><em>BiLQ: An Iterative Method for Nonsymmetric Linear Systems with a Quasi-Minimum Error Property</em></a>, SIAM Journal on Matrix Analysis and Applications, 41(3), pp. 1145–1166, 2020.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/Krylov.jl/blob/94caa506e3363ec846b33c79ccc8889d96055e85/src/bilq.jl#L15-L32">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Krylov.bilqr!-Union{Tuple{S}, Tuple{T}, Tuple{BilqrSolver{T, S}, Any, AbstractVector{T}, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}" href="#Krylov.bilqr!-Union{Tuple{S}, Tuple{T}, Tuple{BilqrSolver{T, S}, Any, AbstractVector{T}, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}"><code>Krylov.bilqr!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">solver = bilqr!(solver::BilqrSolver, args...; kwargs...)</code></pre><p>where <code>args</code> and <code>kwargs</code> are arguments and keyword arguments of <a href="#Krylov.bilqr"><code>bilqr</code></a>.</p><p>See <a href="../../api/#Krylov.BilqrSolver"><code>BilqrSolver</code></a> for more details about the <code>solver</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/Krylov.jl/blob/94caa506e3363ec846b33c79ccc8889d96055e85/src/bilqr.jl#L42-L48">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Krylov.bilqr-Union{Tuple{T}, Tuple{Any, AbstractVector{T}, AbstractVector{T}}} where T&lt;:AbstractFloat" href="#Krylov.bilqr-Union{Tuple{T}, Tuple{Any, AbstractVector{T}, AbstractVector{T}}} where T&lt;:AbstractFloat"><code>Krylov.bilqr</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">(x, y, stats) = bilqr(A, b::AbstractVector{T}, c::AbstractVector{T};
                      atol::T=√eps(T), rtol::T=√eps(T), transfer_to_bicg::Bool=true,
                      itmax::Int=0, verbose::Int=0, history::Bool=false) where T &lt;: AbstractFloat</code></pre><p>Combine BiLQ and QMR to solve adjoint systems.</p><pre><code class="nohighlight hljs">[0  A] [y] = [b]
[Aᵀ 0] [x]   [c]</code></pre><p>The relation <code>bᵀc ≠ 0</code> must be satisfied. BiLQ is used for solving primal system <code>Ax = b</code>. QMR is used for solving dual system <code>Aᵀy = c</code>.</p><p>An option gives the possibility of transferring from the BiLQ point to the BiCG point, when it exists. The transfer is based on the residual norm.</p><p><strong>Reference</strong></p><ul><li>A. Montoison and D. Orban, <a href="https://doi.org/10.1137/19M1290991"><em>BiLQ: An Iterative Method for Nonsymmetric Linear Systems with a Quasi-Minimum Error Property</em></a>, SIAM Journal on Matrix Analysis and Applications, 41(3), pp. 1145–1166, 2020.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/Krylov.jl/blob/94caa506e3363ec846b33c79ccc8889d96055e85/src/bilqr.jl#L15-L35">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Krylov.cg!-Union{Tuple{S}, Tuple{T}, Tuple{CgSolver{T, S}, Any, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}" href="#Krylov.cg!-Union{Tuple{S}, Tuple{T}, Tuple{CgSolver{T, S}, Any, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}"><code>Krylov.cg!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">solver = cg!(solver::CgSolver, args...; kwargs...)</code></pre><p>where <code>args</code> and <code>kwargs</code> are arguments and keyword arguments of <a href="#Krylov.cg"><code>cg</code></a>.</p><p>See <a href="../../api/#Krylov.CgSolver"><code>CgSolver</code></a> for more details about the <code>solver</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/Krylov.jl/blob/94caa506e3363ec846b33c79ccc8889d96055e85/src/cg.jl#L46-L52">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Krylov.cg-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T&lt;:AbstractFloat" href="#Krylov.cg-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T&lt;:AbstractFloat"><code>Krylov.cg</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">(x, stats) = cg(A, b::AbstractVector{T};
                M=I, atol::T=√eps(T), rtol::T=√eps(T), restart::Bool=false,
                itmax::Int=0, radius::T=zero(T), linesearch::Bool=false,
                verbose::Int=0, history::Bool=false) where T &lt;: AbstractFloat</code></pre><p>The conjugate gradient method to solve the symmetric linear system Ax=b.</p><p>The method does <em>not</em> abort if A is not definite.</p><p>A preconditioner M may be provided in the form of a linear operator and is assumed to be symmetric and positive definite. M also indicates the weighted norm in which residuals are measured.</p><p>If <code>itmax=0</code>, the default number of iterations is set to <code>2 * n</code>, with <code>n = length(b)</code>.</p><p><strong>Reference</strong></p><ul><li>M. R. Hestenes and E. Stiefel, <a href="https://doi.org/10.6028/jres.049.044"><em>Methods of conjugate gradients for solving linear systems</em></a>, Journal of Research of the National Bureau of Standards, 49(6), pp. 409–436, 1952.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/Krylov.jl/blob/94caa506e3363ec846b33c79ccc8889d96055e85/src/cg.jl#L19-L39">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Krylov.cg_lanczos!-Union{Tuple{S}, Tuple{T}, Tuple{CgLanczosShiftSolver{T, S}, Any, AbstractVector{T}, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}" href="#Krylov.cg_lanczos!-Union{Tuple{S}, Tuple{T}, Tuple{CgLanczosShiftSolver{T, S}, Any, AbstractVector{T}, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}"><code>Krylov.cg_lanczos!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">solver = cg_lanczos!(solver::CgLanczosShiftSolver, args...; kwargs...)</code></pre><p>where <code>args</code> and <code>kwargs</code> are arguments and keyword arguments of <a href="#Krylov.cg_lanczos"><code>cg_lanczos</code></a> with shifts.</p><p>See <a href="../../api/#Krylov.CgLanczosShiftSolver"><code>CgLanczosShiftSolver</code></a> for more details about the <code>solver</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/Krylov.jl/blob/94caa506e3363ec846b33c79ccc8889d96055e85/src/cg_lanczos.jl#L192-L198">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Krylov.cg_lanczos!-Union{Tuple{S}, Tuple{T}, Tuple{CgLanczosSolver{T, S}, Any, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}" href="#Krylov.cg_lanczos!-Union{Tuple{S}, Tuple{T}, Tuple{CgLanczosSolver{T, S}, Any, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}"><code>Krylov.cg_lanczos!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">solver = cg_lanczos!(solver::CgLanczosSolver, args...; kwargs...)</code></pre><p>where <code>args</code> and <code>kwargs</code> are arguments and keyword arguments of <a href="#Krylov.cg_lanczos"><code>cg_lanczos</code></a> without shifts.</p><p>See <a href="../../api/#Krylov.CgLanczosSolver"><code>CgLanczosSolver</code></a> for more details about the <code>solver</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/Krylov.jl/blob/94caa506e3363ec846b33c79ccc8889d96055e85/src/cg_lanczos.jl#L43-L49">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Krylov.cg_lanczos-Union{Tuple{T}, Tuple{Any, AbstractVector{T}, AbstractVector{T}}} where T&lt;:AbstractFloat" href="#Krylov.cg_lanczos-Union{Tuple{T}, Tuple{Any, AbstractVector{T}, AbstractVector{T}}} where T&lt;:AbstractFloat"><code>Krylov.cg_lanczos</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">(x, stats) = cg_lanczos(A, b::AbstractVector{T}, shifts::AbstractVector{T};
                        M=I, atol::T=√eps(T), rtol::T=√eps(T), itmax::Int=0,
                        check_curvature::Bool=false, verbose::Int=0, history::Bool=false) where T &lt;: AbstractFloat</code></pre><p>The Lanczos version of the conjugate gradient method to solve a family of shifted systems</p><pre><code class="nohighlight hljs">(A + αI) x = b  (α = α₁, ..., αₙ)</code></pre><p>The method does <em>not</em> abort if A + αI is not definite.</p><p>A preconditioner M may be provided in the form of a linear operator and is assumed to be symmetric and positive definite.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/Krylov.jl/blob/94caa506e3363ec846b33c79ccc8889d96055e85/src/cg_lanczos.jl#L170-L184">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Krylov.cg_lanczos-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T&lt;:AbstractFloat" href="#Krylov.cg_lanczos-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T&lt;:AbstractFloat"><code>Krylov.cg_lanczos</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">(x, stats) = cg_lanczos(A, b::AbstractVector{T};
                        M=I, atol::T=√eps(T), rtol::T=√eps(T), itmax::Int=0,
                        check_curvature::Bool=false, verbose::Int=0, history::Bool=false) where T &lt;: AbstractFloat</code></pre><p>The Lanczos version of the conjugate gradient method to solve the symmetric linear system</p><pre><code class="nohighlight hljs">Ax = b</code></pre><p>The method does <em>not</em> abort if A is not definite.</p><p>A preconditioner M may be provided in the form of a linear operator and is assumed to be symmetric and positive definite.</p><p><strong>References</strong></p><ul><li>A. Frommer and P. Maass, <a href="https://doi.org/10.1137/S1064827596313310"><em>Fast CG-Based Methods for Tikhonov-Phillips Regularization</em></a>, SIAM Journal on Scientific Computing, 20(5), pp. 1831–1850, 1999.</li><li>C. C. Paige and M. A. Saunders, <a href="https://doi.org/10.1137/0712047"><em>Solution of Sparse Indefinite Systems of Linear Equations</em></a>, SIAM Journal on Numerical Analysis, 12(4), pp. 617–629, 1975.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/Krylov.jl/blob/94caa506e3363ec846b33c79ccc8889d96055e85/src/cg_lanczos.jl#L17-L36">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Krylov.cgls!-Union{Tuple{S}, Tuple{T}, Tuple{CglsSolver{T, S}, Any, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}" href="#Krylov.cgls!-Union{Tuple{S}, Tuple{T}, Tuple{CglsSolver{T, S}, Any, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}"><code>Krylov.cgls!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">solver = cgls!(solver::CglsSolver, args...; kwargs...)</code></pre><p>where <code>args</code> and <code>kwargs</code> are arguments and keyword arguments of <a href="#Krylov.cgls"><code>cgls</code></a>.</p><p>See <a href="../../api/#Krylov.CglsSolver"><code>CglsSolver</code></a> for more details about the <code>solver</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/Krylov.jl/blob/94caa506e3363ec846b33c79ccc8889d96055e85/src/cgls.jl#L63-L69">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Krylov.cgls-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T&lt;:AbstractFloat" href="#Krylov.cgls-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T&lt;:AbstractFloat"><code>Krylov.cgls</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">(x, stats) = cgls(A, b::AbstractVector{T};
                  M=I, λ::T=zero(T), atol::T=√eps(T), rtol::T=√eps(T),
                  radius::T=zero(T), itmax::Int=0, verbose::Int=0, history::Bool=false) where T &lt;: AbstractFloat</code></pre><p>Solve the regularized linear least-squares problem</p><pre><code class="nohighlight hljs">minimize ‖b - Ax‖₂² + λ‖x‖₂²</code></pre><p>using the Conjugate Gradient (CG) method, where λ ≥ 0 is a regularization parameter. This method is equivalent to applying CG to the normal equations</p><pre><code class="nohighlight hljs">(AᵀA + λI) x = Aᵀb</code></pre><p>but is more stable.</p><p>CGLS produces monotonic residuals ‖r‖₂ but not optimality residuals ‖Aᵀr‖₂. It is formally equivalent to LSQR, though can be slightly less accurate, but simpler to implement.</p><p><strong>References</strong></p><ul><li>M. R. Hestenes and E. Stiefel. <a href="https://doi.org/10.6028/jres.049.044"><em>Methods of conjugate gradients for solving linear systems</em></a>, Journal of Research of the National Bureau of Standards, 49(6), pp. 409–436, 1952.</li><li>A. Björck, T. Elfving and Z. Strakos, <a href="https://doi.org/10.1137/S089547989631202X"><em>Stability of Conjugate Gradient and Lanczos Methods for Linear Least Squares Problems</em></a>, SIAM Journal on Matrix Analysis and Applications, 19(3), pp. 720–736, 1998.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/Krylov.jl/blob/94caa506e3363ec846b33c79ccc8889d96055e85/src/cgls.jl#L32-L56">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Krylov.cgne!-Union{Tuple{S}, Tuple{T}, Tuple{CgneSolver{T, S}, Any, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}" href="#Krylov.cgne!-Union{Tuple{S}, Tuple{T}, Tuple{CgneSolver{T, S}, Any, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}"><code>Krylov.cgne!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">solver = cgne!(solver::CgneSolver, args...; kwargs...)</code></pre><p>where <code>args</code> and <code>kwargs</code> are arguments and keyword arguments of <a href="../solvers/ln/#Krylov.cgne"><code>cgne</code></a>.</p><p>See <a href="../api/#Krylov.CgneSolver"><code>CgneSolver</code></a> for more details about the <code>solver</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/Krylov.jl/blob/94caa506e3363ec846b33c79ccc8889d96055e85/src/cgne.jl#L72-L78">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Krylov.cgne-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T&lt;:AbstractFloat" href="#Krylov.cgne-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T&lt;:AbstractFloat"><code>Krylov.cgne</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">(x, stats) = cgne(A, b::AbstractVector{T};
                  M=I, λ::T=zero(T), atol::T=√eps(T), rtol::T=√eps(T),
                  itmax::Int=0, verbose::Int=0, history::Bool=false) where T &lt;: AbstractFloat</code></pre><p>Solve the consistent linear system</p><pre><code class="nohighlight hljs">Ax + √λs = b</code></pre><p>using the Conjugate Gradient (CG) method, where λ ≥ 0 is a regularization parameter. This method is equivalent to applying CG to the normal equations of the second kind</p><pre><code class="nohighlight hljs">(AAᵀ + λI) y = b</code></pre><p>but is more stable. When λ = 0, this method solves the minimum-norm problem</p><p>min ‖x‖₂  s.t. Ax = b.</p><p>When λ &gt; 0, it solves the problem</p><pre><code class="nohighlight hljs">min ‖(x,s)‖₂  s.t. Ax + √λs = b.</code></pre><p>CGNE produces monotonic errors ‖x-x*‖₂ but not residuals ‖r‖₂. It is formally equivalent to CRAIG, though can be slightly less accurate, but simpler to implement. Only the x-part of the solution is returned.</p><p>A preconditioner M may be provided in the form of a linear operator.</p><p><strong>References</strong></p><ul><li>J. E. Craig, <a href="https://doi.org/10.1002/sapm195534164"><em>The N-step iteration procedures</em></a>, Journal of Mathematics and Physics, 34(1), pp. 64–73, 1955.</li><li>J. E. Craig, <em>Iterations Procedures for Simultaneous Equations</em>, Ph.D. Thesis, Department of Electrical Engineering, MIT, 1954.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/Krylov.jl/blob/94caa506e3363ec846b33c79ccc8889d96055e85/src/cgne.jl#L32-L65">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Krylov.cgs!-Union{Tuple{S}, Tuple{T}, Tuple{CgsSolver{T, S}, Any, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}" href="#Krylov.cgs!-Union{Tuple{S}, Tuple{T}, Tuple{CgsSolver{T, S}, Any, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}"><code>Krylov.cgs!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">solver = cgs!(solver::CgsSolver, args...; kwargs...)</code></pre><p>where <code>args</code> and <code>kwargs</code> are arguments and keyword arguments of <a href="../solvers/unsymmetric/#Krylov.cgs"><code>cgs</code></a>.</p><p>See <a href="../api/#Krylov.CgsSolver"><code>CgsSolver</code></a> for more details about the <code>solver</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/Krylov.jl/blob/94caa506e3363ec846b33c79ccc8889d96055e85/src/cgs.jl#L49-L55">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Krylov.cgs-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T&lt;:AbstractFloat" href="#Krylov.cgs-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T&lt;:AbstractFloat"><code>Krylov.cgs</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">(x, stats) = cgs(A, b::AbstractVector{T}; c::AbstractVector{T}=b,
                 M=I, N=I, atol::T=√eps(T), rtol::T=√eps(T),
                 itmax::Int=0, verbose::Int=0, history::Bool=false) where T &lt;: AbstractFloat</code></pre><p>Solve the consistent linear system Ax = b using conjugate gradient squared algorithm. CGS requires two initial vectors <code>b</code> and <code>c</code>. The relation <code>bᵀc ≠ 0</code> must be satisfied and by default <code>c = b</code>.</p><p>From &quot;Iterative Methods for Sparse Linear Systems (Y. Saad)&quot; :</p><p>«The method is based on a polynomial variant of the conjugate gradients algorithm. Although related to the so-called bi-conjugate gradients (BCG) algorithm, it does not involve adjoint matrix-vector multiplications, and the expected convergence rate is about twice that of the BCG algorithm.</p><p>The Conjugate Gradient Squared algorithm works quite well in many cases. However, one difficulty is that, since the polynomials are squared, rounding errors tend to be more damaging than in the standard BCG algorithm. In particular, very high variations of the residual vectors often cause the residual norms computed to become inaccurate.</p><p>TFQMR and BICGSTAB were developed to remedy this difficulty.»</p><p>This implementation allows a left preconditioner M and a right preconditioner N.</p><p><strong>Reference</strong></p><ul><li>P. Sonneveld, <a href="https://doi.org/10.1137/0910004"><em>CGS, A Fast Lanczos-Type Solver for Nonsymmetric Linear systems</em></a>, SIAM Journal on Scientific and Statistical Computing, 10(1), pp. 36–52, 1989.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/Krylov.jl/blob/94caa506e3363ec846b33c79ccc8889d96055e85/src/cgs.jl#L13-L42">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Krylov.cr!-Union{Tuple{S}, Tuple{T}, Tuple{CrSolver{T, S}, Any, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}" href="#Krylov.cr!-Union{Tuple{S}, Tuple{T}, Tuple{CrSolver{T, S}, Any, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}"><code>Krylov.cr!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">solver = cr!(solver::CrSolver, args...; kwargs...)</code></pre><p>where <code>args</code> and <code>kwargs</code> are arguments and keyword arguments of <a href="#Krylov.cr"><code>cr</code></a>.</p><p>See <a href="../../api/#Krylov.CrSolver"><code>CrSolver</code></a> for more details about the <code>solver</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/Krylov.jl/blob/94caa506e3363ec846b33c79ccc8889d96055e85/src/cr.jl#L45-L51">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Krylov.cr-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T&lt;:AbstractFloat" href="#Krylov.cr-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T&lt;:AbstractFloat"><code>Krylov.cr</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">(x, stats) = cr(A, b::AbstractVector{T};
                M=I, atol::T=√eps(T), rtol::T=√eps(T), γ::T=√eps(T), itmax::Int=0,
                radius::T=zero(T), verbose::Int=0, linesearch::Bool=false, history::Bool=false) where T &lt;: AbstractFloat</code></pre><p>A truncated version of Stiefel’s Conjugate Residual method to solve the symmetric linear system Ax = b or the least-squares problem min ‖b - Ax‖. The matrix A must be positive semi-definite.</p><p>A preconditioner M may be provided in the form of a linear operator and is assumed to be symmetric and positive definite. M also indicates the weighted norm in which residuals are measured.</p><p>In a linesearch context, &#39;linesearch&#39; must be set to &#39;true&#39;.</p><p>If <code>itmax=0</code>, the default number of iterations is set to <code>2 * n</code>, with <code>n = length(b)</code>.</p><p><strong>References</strong></p><ul><li>M. R. Hestenes and E. Stiefel, <a href="https://doi.org/10.6028/jres.049.044"><em>Methods of conjugate gradients for solving linear systems</em></a>, Journal of Research of the National Bureau of Standards, 49(6), pp. 409–436, 1952.</li><li>E. Stiefel, <a href="https://doi.org/10.1007/BF02564277"><em>Relaxationsmethoden bester Strategie zur Losung linearer Gleichungssysteme</em></a>, Commentarii Mathematici Helvetici, 29(1), pp. 157–179, 1955.</li><li>M-A. Dahito and D. Orban, <a href="https://doi.org/10.1137/18M1204255"><em>The Conjugate Residual Method in Linesearch and Trust-Region Methods</em></a>, SIAM Journal on Optimization, 29(3), pp. 1988–2025, 2019.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/Krylov.jl/blob/94caa506e3363ec846b33c79ccc8889d96055e85/src/cr.jl#L17-L38">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Krylov.craig!-Union{Tuple{S}, Tuple{T}, Tuple{CraigSolver{T, S}, Any, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}" href="#Krylov.craig!-Union{Tuple{S}, Tuple{T}, Tuple{CraigSolver{T, S}, Any, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}"><code>Krylov.craig!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">solver = craig!(solver::CraigSolver, args...; kwargs...)</code></pre><p>where <code>args</code> and <code>kwargs</code> are arguments and keyword arguments of <a href="../solvers/ln/#Krylov.craig"><code>craig</code></a>.</p><p>See <a href="../api/#Krylov.CraigSolver"><code>CraigSolver</code></a> for more details about the <code>solver</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/Krylov.jl/blob/94caa506e3363ec846b33c79ccc8889d96055e85/src/craig.jl#L94-L100">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Krylov.craig-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T&lt;:AbstractFloat" href="#Krylov.craig-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T&lt;:AbstractFloat"><code>Krylov.craig</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">(x, y, stats) = craig(A, b::AbstractVector{T};
                      M=I, N=I, sqd::Bool=false, λ::T=zero(T), atol::T=√eps(T),
                      btol::T=√eps(T), rtol::T=√eps(T), conlim::T=1/√eps(T), itmax::Int=0,
                      verbose::Int=0, transfer_to_lsqr::Bool=false, history::Bool=false) where T &lt;: AbstractFloat</code></pre><p>Find the least-norm solution of the consistent linear system</p><pre><code class="nohighlight hljs">Ax + λ²y = b</code></pre><p>using the Golub-Kahan implementation of Craig&#39;s method, where λ ≥ 0 is a regularization parameter. This method is equivalent to CGNE but is more stable.</p><p>For a system in the form Ax = b, Craig&#39;s method is equivalent to applying CG to AAᵀy = b and recovering x = Aᵀy. Note that y are the Lagrange multipliers of the least-norm problem</p><pre><code class="nohighlight hljs">minimize ‖x‖  s.t.  Ax = b.</code></pre><p>If <code>sqd = true</code>, CRAIG solves the symmetric and quasi-definite system</p><pre><code class="nohighlight hljs">[ -F   Aᵀ ] [ x ]   [ 0 ]
[  A   E  ] [ y ] = [ b ],</code></pre><p>where E and F are symmetric and positive definite. The system above represents the optimality conditions of</p><pre><code class="nohighlight hljs">min ‖x‖_F + ‖y‖_E  s.t.  Ax + Ey = b.</code></pre><p>For a symmetric and positive definite matrix <code>K</code>, the K-norm of a vector <code>x</code> is <code>‖x‖²_K = xᵀKx</code>. CRAIG is then equivalent to applying CG to <code>(AF⁻¹Aᵀ + E)y = b</code> with <code>Fx = Aᵀy</code>. Preconditioners M = E⁻¹ ≻ 0 and N = F⁻¹ ≻ 0 may be provided in the form of linear operators.</p><p>If <code>sqd = false</code>, CRAIG solves the symmetric and indefinite system</p><pre><code class="nohighlight hljs">[ -F   Aᵀ ] [ x ]   [ 0 ]
[  A   0  ] [ y ] = [ b ].</code></pre><p>The system above represents the optimality conditions of</p><pre><code class="nohighlight hljs">minimize ‖x‖_F  s.t.  Ax = b.</code></pre><p>In this case, <code>M</code> can still be specified and indicates the weighted norm in which residuals are measured.</p><p>In this implementation, both the x and y-parts of the solution are returned.</p><p><strong>References</strong></p><ul><li>C. C. Paige and M. A. Saunders, <a href="https://doi.org/10.1145/355984.355989"><em>LSQR: An Algorithm for Sparse Linear Equations and Sparse Least Squares</em></a>, ACM Transactions on Mathematical Software, 8(1), pp. 43–71, 1982.</li><li>M. A. Saunders, <a href="https://doi.org/10.1007/BF01739829"><em>Solutions of Sparse Rectangular Systems Using LSQR and CRAIG</em></a>, BIT Numerical Mathematics, 35(4), pp. 588–604, 1995.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/Krylov.jl/blob/94caa506e3363ec846b33c79ccc8889d96055e85/src/craig.jl#L36-L87">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Krylov.craigmr!-Union{Tuple{S}, Tuple{T}, Tuple{CraigmrSolver{T, S}, Any, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}" href="#Krylov.craigmr!-Union{Tuple{S}, Tuple{T}, Tuple{CraigmrSolver{T, S}, Any, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}"><code>Krylov.craigmr!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">solver = craigmr!(solver::CraigmrSolver, args...; kwargs...)</code></pre><p>where <code>args</code> and <code>kwargs</code> are arguments and keyword arguments of <a href="../solvers/ln/#Krylov.craigmr"><code>craigmr</code></a>.</p><p>See <a href="../api/#Krylov.CraigmrSolver"><code>CraigmrSolver</code></a> for more details about the <code>solver</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/Krylov.jl/blob/94caa506e3363ec846b33c79ccc8889d96055e85/src/craigmr.jl#L94-L100">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Krylov.craigmr-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T&lt;:AbstractFloat" href="#Krylov.craigmr-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T&lt;:AbstractFloat"><code>Krylov.craigmr</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">(x, y, stats) = craigmr(A, b::AbstractVector{T};
                        M=I, N=I, sqd::Bool=false, λ::T=zero(T), atol::T=√eps(T),
                        rtol::T=√eps(T), itmax::Int=0, verbose::Int=0, history::Bool=false) where T &lt;: AbstractFloat</code></pre><p>Solve the consistent linear system</p><pre><code class="nohighlight hljs">Ax + λ²y = b</code></pre><p>using the CRAIGMR method, where λ ≥ 0 is a regularization parameter. This method is equivalent to applying the Conjugate Residuals method to the normal equations of the second kind</p><pre><code class="nohighlight hljs">(AAᵀ + λ²I) y = b</code></pre><p>but is more stable. When λ = 0, this method solves the minimum-norm problem</p><pre><code class="nohighlight hljs">min ‖x‖  s.t.  x ∈ argmin ‖Ax - b‖.</code></pre><p>When λ &gt; 0, this method solves the problem</p><pre><code class="nohighlight hljs">min ‖(x,y)‖₂  s.t.  Ax + λ²y = b.</code></pre><p>If <code>sqd = true</code>, CRAIGMR solves the symmetric and quasi-definite system</p><pre><code class="nohighlight hljs">[ -F   Aᵀ ] [ x ]   [ 0 ]
[  A   E  ] [ y ] = [ b ],</code></pre><p>where E and F are symmetric and positive definite. The system above represents the optimality conditions of</p><pre><code class="nohighlight hljs">min ‖x‖_F + ‖y‖_E  s.t.  Ax + Ey = b.</code></pre><p>For a symmetric and positive definite matrix <code>K</code>, the K-norm of a vector <code>x</code> is <code>‖x‖²_K = xᵀKx</code>. CRAIGMR is then equivalent to applying MINRES to <code>(AF⁻¹Aᵀ + E)y = b</code> with <code>Fx = Aᵀy</code>. Preconditioners M = E⁻¹ ≻ 0 and N = F⁻¹ ≻ 0 may be provided in the form of linear operators.</p><p>If <code>sqd = false</code>, CRAIGMR solves the symmetric and indefinite system</p><pre><code class="nohighlight hljs">[ -F   Aᵀ ] [ x ]   [ 0 ]
[  A   0  ] [ y ] = [ b ].</code></pre><p>The system above represents the optimality conditions of</p><pre><code class="nohighlight hljs">min ‖x‖_F  s.t.  Ax = b.</code></pre><p>In this case, <code>M</code> can still be specified and indicates the weighted norm in which residuals are measured.</p><p>CRAIGMR produces monotonic residuals ‖r‖₂. It is formally equivalent to CRMR, though can be slightly more accurate, and intricate to implement. Both the x- and y-parts of the solution are returned.</p><p><strong>References</strong></p><ul><li>D. Orban and M. Arioli. <a href="https://doi.org/10.1137/1.9781611974737"><em>Iterative Solution of Symmetric Quasi-Definite Linear Systems</em></a>, Volume 3 of Spotlights. SIAM, Philadelphia, PA, 2017.</li><li>D. Orban, <a href="https://dx.doi.org/10.13140/RG.2.2.17443.99360"><em>The Projected Golub-Kahan Process for Constrained, Linear Least-Squares Problems</em></a>. Cahier du GERAD G-2014-15, 2014.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/Krylov.jl/blob/94caa506e3363ec846b33c79ccc8889d96055e85/src/craigmr.jl#L30-L87">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Krylov.crls!-Union{Tuple{S}, Tuple{T}, Tuple{CrlsSolver{T, S}, Any, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}" href="#Krylov.crls!-Union{Tuple{S}, Tuple{T}, Tuple{CrlsSolver{T, S}, Any, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}"><code>Krylov.crls!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">solver = crls!(solver::CrlsSolver, args...; kwargs...)</code></pre><p>where <code>args</code> and <code>kwargs</code> are arguments and keyword arguments of <a href="#Krylov.crls"><code>crls</code></a>.</p><p>See <a href="../../api/#Krylov.CrlsSolver"><code>CrlsSolver</code></a> for more details about the <code>solver</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/Krylov.jl/blob/94caa506e3363ec846b33c79ccc8889d96055e85/src/crls.jl#L54-L60">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Krylov.crls-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T&lt;:AbstractFloat" href="#Krylov.crls-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T&lt;:AbstractFloat"><code>Krylov.crls</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">(x, stats) = crls(A, b::AbstractVector{T};
                  M=I, λ::T=zero(T), atol::T=√eps(T), rtol::T=√eps(T),
                  radius::T=zero(T), itmax::Int=0, verbose::Int=0, history::Bool=false) where T &lt;: AbstractFloat</code></pre><p>Solve the linear least-squares problem</p><pre><code class="nohighlight hljs">minimize ‖b - Ax‖₂² + λ‖x‖₂²</code></pre><p>using the Conjugate Residuals (CR) method. This method is equivalent to applying MINRES to the normal equations</p><pre><code class="nohighlight hljs">(AᵀA + λI) x = Aᵀb.</code></pre><p>This implementation recurs the residual r := b - Ax.</p><p>CRLS produces monotonic residuals ‖r‖₂ and optimality residuals ‖Aᵀr‖₂. It is formally equivalent to LSMR, though can be substantially less accurate, but simpler to implement.</p><p><strong>Reference</strong></p><ul><li>D. C.-L. Fong, <em>Minimum-Residual Methods for Sparse, Least-Squares using Golubg-Kahan Bidiagonalization</em>, Ph.D. Thesis, Stanford University, 2011.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/Krylov.jl/blob/94caa506e3363ec846b33c79ccc8889d96055e85/src/crls.jl#L24-L47">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Krylov.crmr!-Union{Tuple{S}, Tuple{T}, Tuple{CrmrSolver{T, S}, Any, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}" href="#Krylov.crmr!-Union{Tuple{S}, Tuple{T}, Tuple{CrmrSolver{T, S}, Any, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}"><code>Krylov.crmr!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">solver = crmr!(solver::CrmrSolver, args...; kwargs...)</code></pre><p>where <code>args</code> and <code>kwargs</code> are arguments and keyword arguments of <a href="../solvers/ln/#Krylov.crmr"><code>crmr</code></a>.</p><p>See <a href="../api/#Krylov.CrmrSolver"><code>CrmrSolver</code></a> for more details about the <code>solver</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/Krylov.jl/blob/94caa506e3363ec846b33c79ccc8889d96055e85/src/crmr.jl#L70-L76">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Krylov.crmr-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T&lt;:AbstractFloat" href="#Krylov.crmr-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T&lt;:AbstractFloat"><code>Krylov.crmr</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">(x, stats) = crmr(A, b::AbstractVector{T};
                  M=I, λ::T=zero(T), atol::T=√eps(T),
                  rtol::T=√eps(T), itmax::Int=0, verbose::Int=0, history::Bool=false) where T &lt;: AbstractFloat</code></pre><p>Solve the consistent linear system</p><pre><code class="nohighlight hljs">Ax + √λs = b</code></pre><p>using the Conjugate Residual (CR) method, where λ ≥ 0 is a regularization parameter. This method is equivalent to applying CR to the normal equations of the second kind</p><pre><code class="nohighlight hljs">(AAᵀ + λI) y = b</code></pre><p>but is more stable. When λ = 0, this method solves the minimum-norm problem</p><pre><code class="nohighlight hljs">min ‖x‖₂  s.t.  x ∈ argmin ‖Ax - b‖₂.</code></pre><p>When λ &gt; 0, this method solves the problem</p><pre><code class="nohighlight hljs">min ‖(x,s)‖₂  s.t. Ax + √λs = b.</code></pre><p>CRMR produces monotonic residuals ‖r‖₂. It is formally equivalent to CRAIG-MR, though can be slightly less accurate, but simpler to implement. Only the x-part of the solution is returned.</p><p>A preconditioner M may be provided in the form of a linear operator.</p><p><strong>References</strong></p><ul><li>D. Orban and M. Arioli, <a href="https://doi.org/10.1137/1.9781611974737"><em>Iterative Solution of Symmetric Quasi-Definite Linear Systems</em></a>, Volume 3 of Spotlights. SIAM, Philadelphia, PA, 2017.</li><li>D. Orban, <a href="https://dx.doi.org/10.13140/RG.2.2.17443.99360"><em>The Projected Golub-Kahan Process for Constrained Linear Least-Squares Problems</em></a>. Cahier du GERAD G-2014-15, 2014.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/Krylov.jl/blob/94caa506e3363ec846b33c79ccc8889d96055e85/src/crmr.jl#L30-L63">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Krylov.diom!-Union{Tuple{S}, Tuple{T}, Tuple{DiomSolver{T, S}, Any, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}" href="#Krylov.diom!-Union{Tuple{S}, Tuple{T}, Tuple{DiomSolver{T, S}, Any, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}"><code>Krylov.diom!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">solver = diom!(solver::DiomSolver, args...; kwargs...)</code></pre><p>where <code>args</code> and <code>kwargs</code> are arguments and keyword arguments of <a href="../solvers/unsymmetric/#Krylov.diom"><code>diom</code></a>.</p><p>Note that the <code>memory</code> keyword argument is the only exception. It&#39;s required to create a <code>DiomSolver</code> and can&#39;t be changed later.</p><p>See <a href="../api/#Krylov.DiomSolver"><code>DiomSolver</code></a> for more details about the <code>solver</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/Krylov.jl/blob/94caa506e3363ec846b33c79ccc8889d96055e85/src/diom.jl#L44-L53">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Krylov.diom-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T&lt;:AbstractFloat" href="#Krylov.diom-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T&lt;:AbstractFloat"><code>Krylov.diom</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">(x, stats) = diom(A, b::AbstractVector{T}; memory::Int=20,
                  M=I, N=I, atol::T=√eps(T), rtol::T=√eps(T),
                  restart::Bool=false, itmax::Int=0,
                  verbose::Int=0, history::Bool=false) where T &lt;: AbstractFloat</code></pre><p>Solve the consistent linear system Ax = b using direct incomplete orthogonalization method.</p><p>DIOM only orthogonalizes the new vectors of the Krylov basis against the <code>memory</code> most recent vectors. If CG is well defined on <code>Ax = b</code> and <code>memory = 2</code>, DIOM is theoretically equivalent to CG. If <code>k ≤ memory</code> where <code>k</code> is the number of iterations, DIOM is theoretically equivalent to FOM. Otherwise, DIOM interpolates between CG and FOM and is similar to CG with partial reorthogonalization.</p><p>An advantage of DIOM is that nonsymmetric or symmetric indefinite or both nonsymmetric and indefinite systems of linear equations can be handled by this single algorithm.</p><p>This implementation allows a left preconditioner M and a right preconditioner N.</p><ul><li>Left  preconditioning : M⁻¹Ax = M⁻¹b</li><li>Right preconditioning : AN⁻¹u = b with x = N⁻¹u</li><li>Split preconditioning : M⁻¹AN⁻¹u = M⁻¹b with x = N⁻¹u</li></ul><p><strong>Reference</strong></p><ul><li>Y. Saad, <a href="https://doi.org/10.1137/0905015"><em>Practical use of some krylov subspace methods for solving indefinite and nonsymmetric linear systems</em></a>, SIAM journal on scientific and statistical computing, 5(1), pp. 203–228, 1984.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/Krylov.jl/blob/94caa506e3363ec846b33c79ccc8889d96055e85/src/diom.jl#L13-L37">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Krylov.dqgmres!-Union{Tuple{S}, Tuple{T}, Tuple{DqgmresSolver{T, S}, Any, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}" href="#Krylov.dqgmres!-Union{Tuple{S}, Tuple{T}, Tuple{DqgmresSolver{T, S}, Any, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}"><code>Krylov.dqgmres!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">solver = dqgmres!(solver::DqgmresSolver, args...; kwargs...)</code></pre><p>where <code>args</code> and <code>kwargs</code> are arguments and keyword arguments of <a href="../solvers/unsymmetric/#Krylov.dqgmres"><code>dqgmres</code></a>.</p><p>Note that the <code>memory</code> keyword argument is the only exception. It&#39;s required to create a <code>DqgmresSolver</code> and can&#39;t be changed later.</p><p>See <a href="../api/#Krylov.DqgmresSolver"><code>DqgmresSolver</code></a> for more details about the <code>solver</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/Krylov.jl/blob/94caa506e3363ec846b33c79ccc8889d96055e85/src/dqgmres.jl#L44-L53">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Krylov.dqgmres-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T&lt;:AbstractFloat" href="#Krylov.dqgmres-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T&lt;:AbstractFloat"><code>Krylov.dqgmres</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">(x, stats) = dqgmres(A, b::AbstractVector{T}; memory::Int=20,
                     M=I, N=I, atol::T=√eps(T), rtol::T=√eps(T),
                     restart::Bool=false, itmax::Int=0,
                     verbose::Int=0, history::Bool=false) where T &lt;: AbstractFloat</code></pre><p>Solve the consistent linear system Ax = b using DQGMRES method.</p><p>DQGMRES algorithm is based on the incomplete Arnoldi orthogonalization process and computes a sequence of approximate solutions with the quasi-minimal residual property.</p><p>DQGMRES only orthogonalizes the new vectors of the Krylov basis against the <code>memory</code> most recent vectors. If MINRES is well defined on <code>Ax = b</code> and <code>memory = 2</code>, DQGMRES is theoretically equivalent to MINRES. If <code>k ≤ memory</code> where <code>k</code> is the number of iterations, DQGMRES is theoretically equivalent to GMRES. Otherwise, DQGMRES interpolates between MINRES and GMRES and is similar to MINRES with partial reorthogonalization.</p><p>This implementation allows a left preconditioner M and a right preconditioner N.</p><ul><li>Left  preconditioning : M⁻¹Ax = M⁻¹b</li><li>Right preconditioning : AN⁻¹u = b with x = N⁻¹u</li><li>Split preconditioning : M⁻¹AN⁻¹u = M⁻¹b with x = N⁻¹u</li></ul><p><strong>Reference</strong></p><ul><li>Y. Saad and K. Wu, <a href="https://doi.org/10.1002/(SICI)1099-1506(199607/08)3:4%3C329::AID-NLA86%3E3.0.CO;2-8"><em>DQGMRES: a quasi minimal residual algorithm based on incomplete orthogonalization</em></a>, Numerical Linear Algebra with Applications, Vol. 3(4), pp. 329–343, 1996.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/Krylov.jl/blob/94caa506e3363ec846b33c79ccc8889d96055e85/src/dqgmres.jl#L13-L37">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Krylov.fom!-Union{Tuple{S}, Tuple{T}, Tuple{FomSolver{T, S}, Any, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}" href="#Krylov.fom!-Union{Tuple{S}, Tuple{T}, Tuple{FomSolver{T, S}, Any, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}"><code>Krylov.fom!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">solver = fom!(solver::FomSolver, args...; kwargs...)</code></pre><p>where <code>args</code> and <code>kwargs</code> are arguments and keyword arguments of <a href="../solvers/unsymmetric/#Krylov.fom"><code>fom</code></a>.</p><p>Note that the <code>memory</code> keyword argument is the only exception. It&#39;s required to create a <code>FomSolver</code> and can&#39;t be changed later.</p><p>See <a href="../api/#Krylov.FomSolver"><code>FomSolver</code></a> for more details about the <code>solver</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/Krylov.jl/blob/94caa506e3363ec846b33c79ccc8889d96055e85/src/fom.jl#L40-L49">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Krylov.fom-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T&lt;:AbstractFloat" href="#Krylov.fom-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T&lt;:AbstractFloat"><code>Krylov.fom</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">(x, stats) = fom(A, b::AbstractVector{T}; memory::Int=20,
                 M=I, N=I, atol::T=√eps(T), rtol::T=√eps(T),
                 reorthogonalization::Bool=false, itmax::Int=0,
                 restart::Bool=false, verbose::Int=0, history::Bool=false) where T &lt;: AbstractFloat</code></pre><p>Solve the linear system Ax = b using FOM method.</p><p>FOM algorithm is based on the Arnoldi process and a Galerkin condition.</p><p>This implementation allows a left preconditioner M and a right preconditioner N.</p><ul><li>Left  preconditioning : M⁻¹Ax = M⁻¹b</li><li>Right preconditioning : AN⁻¹u = b with x = N⁻¹u</li><li>Split preconditioning : M⁻¹AN⁻¹u = M⁻¹b with x = N⁻¹u</li></ul><p>Full reorthogonalization is available with the <code>reorthogonalization</code> option.</p><p><strong>Reference</strong></p><ul><li>Y. Saad, <a href="https://doi.org/10.1090/S0025-5718-1981-0616364-6"><em>Krylov subspace methods for solving unsymmetric linear systems</em></a>, Mathematics of computation, Vol. 37(155), pp. 105–126, 1981.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/Krylov.jl/blob/94caa506e3363ec846b33c79ccc8889d96055e85/src/fom.jl#L13-L33">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Krylov.gmres!-Union{Tuple{S}, Tuple{T}, Tuple{GmresSolver{T, S}, Any, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}" href="#Krylov.gmres!-Union{Tuple{S}, Tuple{T}, Tuple{GmresSolver{T, S}, Any, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}"><code>Krylov.gmres!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">solver = gmres!(solver::GmresSolver, args...; kwargs...)</code></pre><p>where <code>args</code> and <code>kwargs</code> are arguments and keyword arguments of <a href="../solvers/unsymmetric/#Krylov.gmres"><code>gmres</code></a>.</p><p>Note that the <code>memory</code> keyword argument is the only exception. It&#39;s required to create a <code>GmresSolver</code> and can&#39;t be changed later.</p><p>See <a href="../api/#Krylov.GmresSolver"><code>GmresSolver</code></a> for more details about the <code>solver</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/Krylov.jl/blob/94caa506e3363ec846b33c79ccc8889d96055e85/src/gmres.jl#L40-L49">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Krylov.gmres-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T&lt;:AbstractFloat" href="#Krylov.gmres-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T&lt;:AbstractFloat"><code>Krylov.gmres</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">(x, stats) = gmres(A, b::AbstractVector{T}; memory::Int=20,
                   M=I, N=I, atol::T=√eps(T), rtol::T=√eps(T),
                   reorthogonalization::Bool=false, itmax::Int=0,
                   restart::Bool=false, verbose::Int=0, history::Bool=false) where T &lt;: AbstractFloat</code></pre><p>Solve the linear system Ax = b using GMRES method.</p><p>GMRES algorithm is based on the Arnoldi process and computes a sequence of approximate solutions with the minimal residual property.</p><p>This implementation allows a left preconditioner M and a right preconditioner N.</p><ul><li>Left  preconditioning : M⁻¹Ax = M⁻¹b</li><li>Right preconditioning : AN⁻¹u = b with x = N⁻¹u</li><li>Split preconditioning : M⁻¹AN⁻¹u = M⁻¹b with x = N⁻¹u</li></ul><p>Full reorthogonalization is available with the <code>reorthogonalization</code> option.</p><p><strong>Reference</strong></p><ul><li>Y. Saad and M. H. Schultz, <a href="https://doi.org/10.1137/0907058"><em>GMRES: A Generalized Minimal Residual Algorithm for Solving Nonsymmetric Linear Systems</em></a>, SIAM Journal on Scientific and Statistical Computing, Vol. 7(3), pp. 856–869, 1986.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/Krylov.jl/blob/94caa506e3363ec846b33c79ccc8889d96055e85/src/gmres.jl#L13-L33">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Krylov.gpmr!-Union{Tuple{S}, Tuple{T}, Tuple{GpmrSolver{T, S}, Any, Any, AbstractVector{T}, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}" href="#Krylov.gpmr!-Union{Tuple{S}, Tuple{T}, Tuple{GpmrSolver{T, S}, Any, Any, AbstractVector{T}, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}"><code>Krylov.gpmr!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">solver = gpmr!(solver::GpmrSolver, args...; kwargs...)</code></pre><p>where <code>args</code> and <code>kwargs</code> are arguments and keyword arguments of <a href="#Krylov.gpmr"><code>gpmr</code></a>.</p><p>Note that the <code>memory</code> keyword argument is the only exception. It&#39;s required to create a <code>GpmrSolver</code> and can&#39;t be changed later.</p><p>See <a href="../../api/#Krylov.GpmrSolver"><code>GpmrSolver</code></a> for more details about the <code>solver</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/Krylov.jl/blob/94caa506e3363ec846b33c79ccc8889d96055e85/src/gpmr.jl#L67-L76">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Krylov.gpmr-Union{Tuple{T}, Tuple{Any, Any, AbstractVector{T}, AbstractVector{T}}} where T&lt;:AbstractFloat" href="#Krylov.gpmr-Union{Tuple{T}, Tuple{Any, Any, AbstractVector{T}, AbstractVector{T}}} where T&lt;:AbstractFloat"><code>Krylov.gpmr</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">(x, y, stats) = gpmr(A, B, b::AbstractVector{T}, c::AbstractVector{T}; memory::Int=20,
                     C=I, D=I, E=I, F=I, atol::T=√eps(T), rtol::T=√eps(T),
                     gsp::Bool=false, reorthogonalization::Bool=false, itmax::Int=0,
                     restart::Bool=false, λ::T=one(T), μ::T=one(T),
                     verbose::Int=0, history::Bool=false) where T &lt;: AbstractFloat</code></pre><p>GPMR solves the unsymmetric partitioned linear system</p><pre><code class="nohighlight hljs">[ λI   A ] [ x ] = [ b ]
[  B  μI ] [ y ]   [ c ],</code></pre><p>where λ and μ are real numbers. <code>A</code> can have any shape and <code>B</code> has the shape of <code>Aᵀ</code>. <code>A</code>, <code>B</code>, <code>b</code> and <code>c</code> must be all nonzero.</p><p>This implementation allows left and right block diagonal preconditioners</p><pre><code class="nohighlight hljs">[ C    ] [ λM   A ] [ E    ] [ E⁻¹x ] = [ Cb ]
[    D ] [  B  μN ] [    F ] [ F⁻¹y ]   [ Dc ],</code></pre><p>and can solve</p><pre><code class="nohighlight hljs">[ λM   A ] [ x ] = [ b ]
[  B  μN ] [ y ]   [ c ]</code></pre><p>when <code>CE = M⁻¹</code> and <code>DF = N⁻¹</code>.</p><p>By default, GPMR solves unsymmetric linear systems with <code>λ = 1</code> and <code>μ = 1</code>. If <code>gsp = true</code>, <code>λ = 1</code>, <code>μ = 0</code> and the associated generalized saddle point system is solved. <code>λ</code> and <code>μ</code> are also keyword arguments that can be directly modified for more specific problems.</p><p>GPMR is based on the orthogonal Hessenberg reduction process and its relations with the block-Arnoldi process. The residual norm ‖rₖ‖ is monotonically decreasing in GPMR.</p><p>GPMR stops when <code>itmax</code> iterations are reached or when <code>‖rₖ‖ ≤ atol + ‖r₀‖ * rtol</code>. <code>atol</code> is an absolute tolerance and <code>rtol</code> is a relative tolerance.</p><p>Full reorthogonalization is available with the <code>reorthogonalization</code> option.</p><p>Additional details can be displayed if verbose mode is enabled (verbose &gt; 0). Information will be displayed every <code>verbose</code> iterations.</p><p><strong>Reference</strong></p><ul><li>A. Montoison and D. Orban, <a href="https://dx.doi.org/10.13140/RG.2.2.24069.68326"><em>GPMR: An Iterative Method for Unsymmetric Partitioned Linear Systems</em></a>, Cahier du GERAD G-2021-62, GERAD, Montréal, 2021.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/Krylov.jl/blob/94caa506e3363ec846b33c79ccc8889d96055e85/src/gpmr.jl#L14-L60">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Krylov.kones-Tuple{Any, Any}" href="#Krylov.kones-Tuple{Any, Any}"><code>Krylov.kones</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">v = kones(S, n)</code></pre><p>Create an AbstractVector of storage type <code>S</code> of length <code>n</code> only composed of one.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/Krylov.jl/blob/94caa506e3363ec846b33c79ccc8889d96055e85/src/krylov_utils.jl#L192-L196">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Krylov.ktypeof-Tuple{AbstractVector}" href="#Krylov.ktypeof-Tuple{AbstractVector}"><code>Krylov.ktypeof</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">S = ktypeof(v)</code></pre><p>Return a dense storage type <code>S</code> based on the type of <code>v</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/Krylov.jl/blob/94caa506e3363ec846b33c79ccc8889d96055e85/src/krylov_utils.jl#L169-L173">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Krylov.kzeros-Tuple{Any, Any}" href="#Krylov.kzeros-Tuple{Any, Any}"><code>Krylov.kzeros</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">v = kzeros(S, n)</code></pre><p>Create an AbstractVector of storage type <code>S</code> of length <code>n</code> only composed of zero.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/Krylov.jl/blob/94caa506e3363ec846b33c79ccc8889d96055e85/src/krylov_utils.jl#L185-L189">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Krylov.lnlq!-Union{Tuple{S}, Tuple{T}, Tuple{LnlqSolver{T, S}, Any, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}" href="#Krylov.lnlq!-Union{Tuple{S}, Tuple{T}, Tuple{LnlqSolver{T, S}, Any, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}"><code>Krylov.lnlq!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">solver = lnlq!(solver::LnlqSolver, args...; kwargs...)</code></pre><p>where <code>args</code> and <code>kwargs</code> are arguments and keyword arguments of <a href="../solvers/ln/#Krylov.lnlq"><code>lnlq</code></a>.</p><p>See <a href="../api/#Krylov.LnlqSolver"><code>LnlqSolver</code></a> for more details about the <code>solver</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/Krylov.jl/blob/94caa506e3363ec846b33c79ccc8889d96055e85/src/lnlq.jl#L86-L92">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Krylov.lnlq-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T&lt;:AbstractFloat" href="#Krylov.lnlq-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T&lt;:AbstractFloat"><code>Krylov.lnlq</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">(x, y, stats) = lnlq(A, b::AbstractVector{T};
                     M=I, N=I, sqd::Bool=false, λ::T=zero(T), σ::T=zero(T),
                     atol::T=√eps(T), rtol::T=√eps(T), etolx::T=√eps(T), etoly::T=√eps(T), itmax::Int=0,
                     transfer_to_craig::Bool=true, verbose::Int=0, history::Bool=false) where T &lt;: AbstractFloat</code></pre><p>Find the least-norm solution of the consistent linear system</p><pre><code class="nohighlight hljs">Ax + λ²y = b</code></pre><p>using the LNLQ method, where λ ≥ 0 is a regularization parameter.</p><p>For a system in the form Ax = b, LNLQ method is equivalent to applying SYMMLQ to AAᵀy = b and recovering x = Aᵀy but is more stable. Note that y are the Lagrange multipliers of the least-norm problem</p><pre><code class="nohighlight hljs">minimize ‖x‖  s.t.  Ax = b.</code></pre><p>If <code>sqd = true</code>, LNLQ solves the symmetric and quasi-definite system</p><pre><code class="nohighlight hljs">[ -F   Aᵀ ] [ x ]   [ 0 ]
[  A   E  ] [ y ] = [ b ],</code></pre><p>where E and F are symmetric and positive definite. The system above represents the optimality conditions of</p><pre><code class="nohighlight hljs">min ‖x‖_F + ‖y‖_E  s.t.  Ax + Ey = b.</code></pre><p>For a symmetric and positive definite matrix <code>K</code>, the K-norm of a vector <code>x</code> is <code>‖x‖²_K = xᵀKx</code>. LNLQ is then equivalent to applying SYMMLQ to <code>(AF⁻¹Aᵀ + E)y = b</code> with <code>Fx = Aᵀy</code>. Preconditioners M = E⁻¹ ≻ 0 and N = F⁻¹ ≻ 0 may be provided in the form of linear operators.</p><p>If <code>sqd = false</code>, LNLQ solves the symmetric and indefinite system</p><pre><code class="nohighlight hljs">[ -F   Aᵀ ] [ x ]   [ 0 ]
[  A   0  ] [ y ] = [ b ].</code></pre><p>The system above represents the optimality conditions of</p><pre><code class="nohighlight hljs">minimize ‖x‖_F  s.t.  Ax = b.</code></pre><p>In this case, <code>M</code> can still be specified and indicates the weighted norm in which residuals are measured.</p><p>In this implementation, both the x and y-parts of the solution are returned.</p><p><code>etolx</code> and <code>etoly</code> are tolerances on the upper bound of the distance to the solution ‖x-xₛ‖ and ‖y-yₛ‖, respectively. The bound is valid if λ&gt;0 or σ&gt;0 where σ should be strictly smaller than the smallest positive singular value. For instance σ:=(1-1e-7)σₘᵢₙ .</p><p><strong>Reference</strong></p><ul><li>R. Estrin, D. Orban, M.A. Saunders, <a href="https://doi.org/10.1137/18M1194948"><em>LNLQ: An Iterative Method for Least-Norm Problems with an Error Minimization Property</em></a>, SIAM Journal on Matrix Analysis and Applications, 40(3), pp. 1102–1124, 2019.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/Krylov.jl/blob/94caa506e3363ec846b33c79ccc8889d96055e85/src/lnlq.jl#L27-L79">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Krylov.lslq!-Union{Tuple{S}, Tuple{T}, Tuple{LslqSolver{T, S}, Any, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}" href="#Krylov.lslq!-Union{Tuple{S}, Tuple{T}, Tuple{LslqSolver{T, S}, Any, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}"><code>Krylov.lslq!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">solver = lslq!(solver::LslqSolver, args...; kwargs...)</code></pre><p>where <code>args</code> and <code>kwargs</code> are arguments and keyword arguments of <a href="#Krylov.lslq"><code>lslq</code></a>.</p><p>See <a href="../../api/#Krylov.LslqSolver"><code>LslqSolver</code></a> for more details about the <code>solver</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/Krylov.jl/blob/94caa506e3363ec846b33c79ccc8889d96055e85/src/lslq.jl#L136-L142">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Krylov.lslq-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T&lt;:AbstractFloat" href="#Krylov.lslq-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T&lt;:AbstractFloat"><code>Krylov.lslq</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">(x, stats) = lslq(A, b::AbstractVector{T};
                  M=I, N=I, sqd::Bool=false, λ::T=zero(T),
                  atol::T=√eps(T), btol::T=√eps(T), etol::T=√eps(T),
                  window::Int=5, utol::T=√eps(T), itmax::Int=0,
                  σ::T=zero(T), transfer_to_lsqr::Bool=false, 
                  conlim::T=1/√eps(T), verbose::Int=0, history::Bool=false) where T &lt;: AbstractFloat</code></pre><p>Solve the regularized linear least-squares problem</p><pre><code class="nohighlight hljs">minimize ‖b - Ax‖₂² + λ²‖x‖₂²</code></pre><p>using the LSLQ method, where λ ≥ 0 is a regularization parameter. LSLQ is formally equivalent to applying SYMMLQ to the normal equations</p><pre><code class="nohighlight hljs">(AᵀA + λ²I) x = Aᵀb</code></pre><p>but is more stable.</p><p><strong>Main features</strong></p><ul><li>the solution estimate is updated along orthogonal directions</li><li>the norm of the solution estimate ‖xᴸₖ‖₂ is increasing</li><li>the error ‖eₖ‖₂ := ‖xᴸₖ - x*‖₂ is decreasing</li><li>it is possible to transition cheaply from the LSLQ iterate to the LSQR iterate if there is an advantage (there always is in terms of error)</li><li>if <code>A</code> is rank deficient, identify the minimum least-squares solution</li></ul><p><strong>Optional arguments</strong></p><ul><li><code>M</code>: a symmetric and positive definite dual preconditioner</li><li><code>N</code>: a symmetric and positive definite primal preconditioner</li><li><code>sqd</code> indicates whether or not we are solving a symmetric and quasi-definite augmented system</li></ul><p>If <code>sqd = true</code>, we solve the symmetric and quasi-definite system</p><pre><code class="nohighlight hljs">[ E    A ] [ r ]   [ b ]
[ Aᵀ  -F ] [ x ] = [ 0 ],</code></pre><p>where E and F are symmetric and positive definite. The system above represents the optimality conditions of</p><pre><code class="nohighlight hljs">minimize ‖b - Ax‖²_E⁻¹ + ‖x‖²_F.</code></pre><p>For a symmetric and positive definite matrix <code>K</code>, the K-norm of a vector <code>x</code> is <code>‖x‖²_K = xᵀKx</code>. LSLQ is then equivalent to applying SYMMLQ to <code>(AᵀE⁻¹A + F)x = AᵀE⁻¹b</code> with <code>r = E⁻¹(b - Ax)</code>. Preconditioners M = E⁻¹ ≻ 0 and N = F⁻¹ ≻ 0 may be provided in the form of linear operators.</p><p>If <code>sqd</code> is set to <code>false</code> (the default), we solve the symmetric and indefinite system</p><pre><code class="nohighlight hljs">[ E    A ] [ r ]   [ b ]
[ Aᵀ   0 ] [ x ] = [ 0 ].</code></pre><p>The system above represents the optimality conditions of</p><pre><code class="nohighlight hljs">minimize ‖b - Ax‖²_E⁻¹.</code></pre><p>In this case, <code>N</code> can still be specified and indicates the weighted norm in which <code>x</code> and <code>Aᵀr</code> should be measured. <code>r</code> can be recovered by computing <code>E⁻¹(b - Ax)</code>.</p><ul><li><code>λ</code> is a regularization parameter (see the problem statement above)</li><li><code>σ</code> is an underestimate of the smallest nonzero singular value of <code>A</code>–-setting <code>σ</code> too large will result in an error in the course of the iterations</li><li><code>atol</code> is a stopping tolerance based on the residual</li><li><code>btol</code> is a stopping tolerance used to detect zero-residual problems</li><li><code>etol</code> is a stopping tolerance based on the lower bound on the error</li><li><code>window</code> is the number of iterations used to accumulate a lower bound on the error</li><li><code>utol</code> is a stopping tolerance based on the upper bound on the error</li><li><code>transfer_to_lsqr</code> return the CG solution estimate (i.e., the LSQR point) instead of the LQ estimate</li><li><code>itmax</code> is the maximum number of iterations (0 means no imposed limit)</li><li><code>conlim</code> is the limit on the estimated condition number of <code>A</code> beyond which the solution will be abandoned</li><li><code>verbose</code> determines verbosity.</li></ul><p><strong>Return values</strong></p><p><code>lslq</code> returns the tuple <code>(x, stats)</code> where</p><ul><li><p><code>x</code> is the LQ solution estimate</p></li><li><p><code>stats</code> collects other statistics on the run in a LSLQStats</p></li><li><p><code>stats.err_lbnds</code> is a vector of lower bounds on the LQ error–-the vector is empty if <code>window</code> is set to zero</p></li><li><p><code>stats.err_ubnds_lq</code> is a vector of upper bounds on the LQ error–-the vector is empty if <code>σ == 0</code> is left at zero</p></li><li><p><code>stats.err_ubnds_cg</code> is a vector of upper bounds on the CG error–-the vector is empty if <code>σ == 0</code> is left at zero</p></li><li><p><code>stats.error_with_bnd</code> is a boolean indicating whether there was an error in the upper bounds computation (cancellation errors, too large σ ...)</p></li></ul><p><strong>Stopping conditions</strong></p><p>The iterations stop as soon as one of the following conditions holds true:</p><ul><li>the optimality residual is sufficiently small (<code>stats.status = &quot;found approximate minimum least-squares solution&quot;</code>) in the sense that either<ul><li>‖Aᵀr‖ / (‖A‖ ‖r‖) ≤ atol, or</li><li>1 + ‖Aᵀr‖ / (‖A‖ ‖r‖) ≤ 1</li></ul></li><li>an approximate zero-residual solution has been found (<code>stats.status = &quot;found approximate zero-residual solution&quot;</code>) in the sense that either<ul><li>‖r‖ / ‖b‖ ≤ btol + atol ‖A‖ * ‖xᴸ‖ / ‖b‖, or</li><li>1 + ‖r‖ / ‖b‖ ≤ 1</li></ul></li><li>the estimated condition number of <code>A</code> is too large in the sense that either<ul><li>1/cond(A) ≤ 1/conlim (<code>stats.status = &quot;condition number exceeds tolerance&quot;</code>), or</li><li>1 + 1/cond(A) ≤ 1 (<code>stats.status = &quot;condition number seems too large for this machine&quot;</code>)</li></ul></li><li>the lower bound on the LQ forward error is less than etol * ‖xᴸ‖</li><li>the upper bound on the CG forward error is less than utol * ‖xᶜ‖</li></ul><p><strong>References</strong></p><ul><li>R. Estrin, D. Orban and M. A. Saunders, <a href="https://doi.org/10.1137/16M1094816"><em>Euclidean-norm error bounds for SYMMLQ and CG</em></a>, SIAM Journal on Matrix Analysis and Applications, 40(1), pp. 235–253, 2019.</li><li>R. Estrin, D. Orban and M. A. Saunders, <a href="https://doi.org/10.1137/17M1113552"><em>LSLQ: An Iterative Method for Linear Least-Squares with an Error Minimization Property</em></a>, SIAM Journal on Matrix Analysis and Applications, 40(1), pp. 254–275, 2019.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/Krylov.jl/blob/94caa506e3363ec846b33c79ccc8889d96055e85/src/lslq.jl#L25-L129">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Krylov.lsmr!-Union{Tuple{S}, Tuple{T}, Tuple{LsmrSolver{T, S}, Any, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}" href="#Krylov.lsmr!-Union{Tuple{S}, Tuple{T}, Tuple{LsmrSolver{T, S}, Any, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}"><code>Krylov.lsmr!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">solver = lsmr!(solver::LsmrSolver, args...; kwargs...)</code></pre><p>where <code>args</code> and <code>kwargs</code> are arguments and keyword arguments of <a href="#Krylov.lsmr"><code>lsmr</code></a>.</p><p>See <a href="../../api/#Krylov.LsmrSolver"><code>LsmrSolver</code></a> for more details about the <code>solver</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/Krylov.jl/blob/94caa506e3363ec846b33c79ccc8889d96055e85/src/lsmr.jl#L100-L106">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Krylov.lsmr-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T&lt;:AbstractFloat" href="#Krylov.lsmr-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T&lt;:AbstractFloat"><code>Krylov.lsmr</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">(x, stats) = lsmr(A, b::AbstractVector{T};
                  M=I, N=I, sqd::Bool=false,
                  λ::T=zero(T), axtol::T=√eps(T), btol::T=√eps(T),
                  atol::T=zero(T), rtol::T=zero(T),
                  etol::T=√eps(T), window::Int=5,
                  itmax::Int=0, conlim::T=1/√eps(T),
                  radius::T=zero(T), verbose::Int=0,
                  history::Bool=false, callback::Function=(args...) -&gt; false) where T &lt;: AbstractFloat</code></pre><p>Solve the regularized linear least-squares problem</p><pre><code class="nohighlight hljs">minimize ‖b - Ax‖₂² + λ²‖x‖₂²</code></pre><p>using the LSMR method, where λ ≥ 0 is a regularization parameter. LSMR is formally equivalent to applying MINRES to the normal equations</p><pre><code class="nohighlight hljs">(AᵀA + λ²I) x = Aᵀb</code></pre><p>(and therefore to CRLS) but is more stable.</p><p>LSMR produces monotonic residuals ‖r‖₂ and optimality residuals ‖Aᵀr‖₂. It is formally equivalent to CRLS, though can be substantially more accurate.</p><p>LSMR can be also used to find a null vector of a singular matrix A by solving the problem <code>min ‖Aᵀx - b‖</code> with any nonzero vector <code>b</code>. At a minimizer, the residual vector <code>r = b - Aᵀx</code> will satisfy <code>Ar = 0</code>.</p><p>Preconditioners M and N may be provided in the form of linear operators and are assumed to be symmetric and positive definite. If <code>sqd</code> is set to <code>true</code>, we solve the symmetric and quasi-definite system</p><pre><code class="nohighlight hljs">[ E    A ] [ r ]   [ b ]
[ Aᵀ  -F ] [ x ] = [ 0 ],</code></pre><p>where E and F are symmetric and positive definite. The system above represents the optimality conditions of</p><pre><code class="nohighlight hljs">minimize ‖b - Ax‖²_E⁻¹ + ‖x‖²_F.</code></pre><p>For a symmetric and positive definite matrix <code>K</code>, the K-norm of a vector <code>x</code> is <code>‖x‖²_K = xᵀKx</code>. LSMR is then equivalent to applying MINRES to <code>(AᵀE⁻¹A + F)x = AᵀE⁻¹b</code> with <code>r = E⁻¹(b - Ax)</code>. Preconditioners M = E⁻¹ ≻ 0 and N = F⁻¹ ≻ 0 may be provided in the form of linear operators.</p><p>If <code>sqd</code> is set to <code>false</code> (the default), we solve the symmetric and indefinite system</p><pre><code class="nohighlight hljs">[ E    A ] [ r ]   [ b ]
[ Aᵀ   0 ] [ x ] = [ 0 ].</code></pre><p>The system above represents the optimality conditions of</p><pre><code class="nohighlight hljs">minimize ‖b - Ax‖²_E⁻¹.</code></pre><p>In this case, <code>N</code> can still be specified and indicates the weighted norm in which <code>x</code> and <code>Aᵀr</code> should be measured. <code>r</code> can be recovered by computing <code>E⁻¹(b - Ax)</code>.</p><p>The callback is called as <code>callback(solver, iter)</code> and should return <code>true</code> if the main loop should terminate, and <code>false</code> otherwise.</p><p>Note that <code>history</code> should be set to <code>true</code> to have access to <code>rNorms</code> and <code>ArNorms</code> in the callback.</p><p><strong>Reference</strong></p><ul><li>D. C.-L. Fong and M. A. Saunders, <a href="https://doi.org/10.1137/10079687X"><em>LSMR: An Iterative Algorithm for Sparse Least Squares Problems</em></a>, SIAM Journal on Scientific Computing, 33(5), pp. 2950–2971, 2011.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/Krylov.jl/blob/94caa506e3363ec846b33c79ccc8889d96055e85/src/lsmr.jl#L28-L93">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Krylov.lsqr!-Union{Tuple{S}, Tuple{T}, Tuple{LsqrSolver{T, S}, Any, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}" href="#Krylov.lsqr!-Union{Tuple{S}, Tuple{T}, Tuple{LsqrSolver{T, S}, Any, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}"><code>Krylov.lsqr!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">solver = lsqr!(solver::LsqrSolver, args...; kwargs...)</code></pre><p>where <code>args</code> and <code>kwargs</code> are arguments and keyword arguments of <a href="#Krylov.lsqr"><code>lsqr</code></a>.</p><p>See <a href="../../api/#Krylov.LsqrSolver"><code>LsqrSolver</code></a> for more details about the <code>solver</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/Krylov.jl/blob/94caa506e3363ec846b33c79ccc8889d96055e85/src/lsqr.jl#L90-L96">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Krylov.lsqr-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T&lt;:AbstractFloat" href="#Krylov.lsqr-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T&lt;:AbstractFloat"><code>Krylov.lsqr</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">(x, stats) = lsqr(A, b::AbstractVector{T};
                  M=I, N=I, sqd::Bool=false,
                  λ::T=zero(T), axtol::T=√eps(T), btol::T=√eps(T),
                  atol::T=zero(T), rtol::T=zero(T),
                  etol::T=√eps(T), window::Int=5,
                  itmax::Int=0, conlim::T=1/√eps(T),
                  radius::T=zero(T), verbose::Int=0, history::Bool=false) where T &lt;: AbstractFloat</code></pre><p>Solve the regularized linear least-squares problem</p><pre><code class="nohighlight hljs">minimize ‖b - Ax‖₂² + λ²‖x‖₂²</code></pre><p>using the LSQR method, where λ ≥ 0 is a regularization parameter. LSQR is formally equivalent to applying CG to the normal equations</p><pre><code class="nohighlight hljs">(AᵀA + λ²I) x = Aᵀb</code></pre><p>(and therefore to CGLS) but is more stable.</p><p>LSQR produces monotonic residuals ‖r‖₂ but not optimality residuals ‖Aᵀr‖₂. It is formally equivalent to CGLS, though can be slightly more accurate.</p><p>Preconditioners M and N may be provided in the form of linear operators and are assumed to be symmetric and positive definite. If <code>sqd</code> is set to <code>true</code>, we solve the symmetric and quasi-definite system</p><pre><code class="nohighlight hljs">[ E    A ] [ r ]   [ b ]
[ Aᵀ  -F ] [ x ] = [ 0 ],</code></pre><p>where E and F are symmetric and positive definite. The system above represents the optimality conditions of</p><pre><code class="nohighlight hljs">minimize ‖b - Ax‖²_E⁻¹ + ‖x‖²_F.</code></pre><p>For a symmetric and positive definite matrix <code>K</code>, the K-norm of a vector <code>x</code> is <code>‖x‖²_K = xᵀKx</code>. LSQR is then equivalent to applying CG to <code>(AᵀE⁻¹A + F)x = AᵀE⁻¹b</code> with <code>r = E⁻¹(b - Ax)</code>. Preconditioners M = E⁻¹ ≻ 0 and N = F⁻¹ ≻ 0 may be provided in the form of linear operators.</p><p>If <code>sqd</code> is set to <code>false</code> (the default), we solve the symmetric and indefinite system</p><pre><code class="nohighlight hljs">[ E    A ] [ r ]   [ b ]
[ Aᵀ   0 ] [ x ] = [ 0 ].</code></pre><p>The system above represents the optimality conditions of</p><pre><code class="nohighlight hljs">minimize ‖b - Ax‖²_E⁻¹.</code></pre><p>In this case, <code>N</code> can still be specified and indicates the weighted norm in which <code>x</code> and <code>Aᵀr</code> should be measured. <code>r</code> can be recovered by computing <code>E⁻¹(b - Ax)</code>.</p><p><strong>Reference</strong></p><ul><li>C. C. Paige and M. A. Saunders, <a href="https://doi.org/10.1145/355984.355989"><em>LSQR: An Algorithm for Sparse Linear Equations and Sparse Least Squares</em></a>, ACM Transactions on Mathematical Software, 8(1), pp. 43–71, 1982.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/Krylov.jl/blob/94caa506e3363ec846b33c79ccc8889d96055e85/src/lsqr.jl#L28-L83">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Krylov.minres!-Union{Tuple{S}, Tuple{T}, Tuple{MinresSolver{T, S}, Any, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}" href="#Krylov.minres!-Union{Tuple{S}, Tuple{T}, Tuple{MinresSolver{T, S}, Any, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}"><code>Krylov.minres!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">solver = minres!(solver::MinresSolver, args...; kwargs...)</code></pre><p>where <code>args</code> and <code>kwargs</code> are arguments and keyword arguments of <a href="../solvers/sid/#Krylov.minres"><code>minres</code></a>.</p><p>See <a href="../api/#Krylov.MinresSolver"><code>MinresSolver</code></a> for more details about the <code>solver</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/Krylov.jl/blob/94caa506e3363ec846b33c79ccc8889d96055e85/src/minres.jl#L64-L70">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Krylov.minres-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T&lt;:AbstractFloat" href="#Krylov.minres-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T&lt;:AbstractFloat"><code>Krylov.minres</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">(x, stats) = minres(A, b::AbstractVector{T};
                    M=I, λ::T=zero(T), atol::T=√eps(T)/100,
                    rtol::T=√eps(T)/100, ratol :: T=zero(T), 
                    rrtol :: T=zero(T), etol::T=√eps(T),
                    window::Int=5, itmax::Int=0,
                    conlim::T=1/√eps(T), restart::Bool=false,
                    verbose::Int=0, history::Bool=false) where T &lt;: AbstractFloat</code></pre><p>Solve the shifted linear least-squares problem</p><pre><code class="nohighlight hljs">minimize ‖b - (A + λI)x‖₂²</code></pre><p>or the shifted linear system</p><pre><code class="nohighlight hljs">(A + λI) x = b</code></pre><p>using the MINRES method, where λ ≥ 0 is a shift parameter, where A is square and symmetric.</p><p>MINRES is formally equivalent to applying CR to Ax=b when A is positive definite, but is typically more stable and also applies to the case where A is indefinite.</p><p>MINRES produces monotonic residuals ‖r‖₂ and optimality residuals ‖Aᵀr‖₂.</p><p>A preconditioner M may be provided in the form of a linear operator and is assumed to be symmetric and positive definite.</p><p><strong>Reference</strong></p><ul><li>C. C. Paige and M. A. Saunders, <a href="https://doi.org/10.1137/0712047"><em>Solution of Sparse Indefinite Systems of Linear Equations</em></a>, SIAM Journal on Numerical Analysis, 12(4), pp. 617–629, 1975.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/Krylov.jl/blob/94caa506e3363ec846b33c79ccc8889d96055e85/src/minres.jl#L25-L57">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Krylov.minres_qlp!-Union{Tuple{S}, Tuple{T}, Tuple{MinresQlpSolver{T, S}, Any, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}" href="#Krylov.minres_qlp!-Union{Tuple{S}, Tuple{T}, Tuple{MinresQlpSolver{T, S}, Any, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}"><code>Krylov.minres_qlp!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">solver = minres_qlp!(solver::MinresQlpSolver, args...; kwargs...)</code></pre><p>where <code>args</code> and <code>kwargs</code> are arguments and keyword arguments of <a href="../solvers/sid/#Krylov.minres_qlp"><code>minres_qlp</code></a>.</p><p>See <a href="../api/#Krylov.MinresQlpSolver"><code>MinresQlpSolver</code></a> for more details about the <code>solver</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/Krylov.jl/blob/94caa506e3363ec846b33c79ccc8889d96055e85/src/minres_qlp.jl#L45-L51">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Krylov.minres_qlp-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T&lt;:AbstractFloat" href="#Krylov.minres_qlp-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T&lt;:AbstractFloat"><code>Krylov.minres_qlp</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">(x, stats) = minres_qlp(A, b::AbstractVector{T};
                        M=I, atol::T=√eps(T), rtol::T=√eps(T), λ::T=zero(T),
                        itmax::Int=0, restart::Bool=false,
                        verbose::Int=0, history::Bool=false) where T &lt;: AbstractFloat</code></pre><p>MINRES-QLP is the only method based on the Lanczos process that returns the minimum-norm solution on singular inconsistent systems (A + λI)x = b, where λ is a shift parameter. It is significantly more complex but can be more reliable than MINRES when A is ill-conditioned.</p><p>A preconditioner M may be provided in the form of a linear operator and is assumed to be symmetric and positive definite. M also indicates the weighted norm in which residuals are measured.</p><p><strong>References</strong></p><ul><li>S.-C. T. Choi, <em>Iterative methods for singular linear equations and least-squares problems</em>, Ph.D. thesis, ICME, Stanford University, 2006.</li><li>S.-C. T. Choi, C. C. Paige and M. A. Saunders, <a href="https://doi.org/10.1137/100787921"><em>MINRES-QLP: A Krylov subspace method for indefinite or singular symmetric systems</em></a>, SIAM Journal on Scientific Computing, Vol. 33(4), pp. 1810–1836, 2011.</li><li>S.-C. T. Choi and M. A. Saunders, <a href="https://doi.org/10.1145/2527267"><em>Algorithm 937: MINRES-QLP for symmetric and Hermitian linear equations and least-squares problems</em></a>, ACM Transactions on Mathematical Software, 40(2), pp. 1–12, 2014.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/Krylov.jl/blob/94caa506e3363ec846b33c79ccc8889d96055e85/src/minres_qlp.jl#L19-L38">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Krylov.niterations" href="#Krylov.niterations"><code>Krylov.niterations</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">niterations(solver)</code></pre><p>Return the number of iterations performed by the Krylov method associated to <code>solver</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/Krylov.jl/blob/94caa506e3363ec846b33c79ccc8889d96055e85/src/krylov_solvers.jl#L1532-L1536">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Krylov.qmr!-Union{Tuple{S}, Tuple{T}, Tuple{QmrSolver{T, S}, Any, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}" href="#Krylov.qmr!-Union{Tuple{S}, Tuple{T}, Tuple{QmrSolver{T, S}, Any, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}"><code>Krylov.qmr!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">solver = qmr!(solver::QmrSolver, args...; kwargs...)</code></pre><p>where <code>args</code> and <code>kwargs</code> are arguments and keyword arguments of <a href="../solvers/unsymmetric/#Krylov.qmr"><code>qmr</code></a>.</p><p>See <a href="../api/#Krylov.QmrSolver"><code>QmrSolver</code></a> for more details about the <code>solver</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/Krylov.jl/blob/94caa506e3363ec846b33c79ccc8889d96055e85/src/qmr.jl#L46-L52">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Krylov.qmr-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T&lt;:AbstractFloat" href="#Krylov.qmr-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T&lt;:AbstractFloat"><code>Krylov.qmr</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">(x, stats) = qmr(A, b::AbstractVector{T}; c::AbstractVector{T}=b,
                 atol::T=√eps(T), rtol::T=√eps(T),
                 itmax::Int=0, verbose::Int=0, history::Bool=false) where T &lt;: AbstractFloat</code></pre><p>Solve the square linear system Ax = b using the QMR method.</p><p>QMR is based on the Lanczos biorthogonalization process and requires two initial vectors <code>b</code> and <code>c</code>. The relation <code>bᵀc ≠ 0</code> must be satisfied and by default <code>c = b</code>. When <code>A</code> is symmetric and <code>b = c</code>, QMR is equivalent to MINRES.</p><p><strong>References</strong></p><ul><li>R. W. Freund and N. M. Nachtigal, <a href="https://doi.org/10.1007/BF01385726"><em>QMR : a quasi-minimal residual method for non-Hermitian linear systems</em></a>, Numerische mathematik, Vol. 60(1), pp. 315–339, 1991.</li><li>R. W. Freund and N. M. Nachtigal, <a href="https://doi.org/10.1137/0915022"><em>An implementation of the QMR method based on coupled two-term recurrences</em></a>, SIAM Journal on Scientific Computing, Vol. 15(2), pp. 313–337, 1994.</li><li>A. Montoison and D. Orban, <a href="https://doi.org/10.1137/19M1290991"><em>BiLQ: An Iterative Method for Nonsymmetric Linear Systems with a Quasi-Minimum Error Property</em></a>, SIAM Journal on Matrix Analysis and Applications, 41(3), pp. 1145–1166, 2020.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/Krylov.jl/blob/94caa506e3363ec846b33c79ccc8889d96055e85/src/qmr.jl#L23-L39">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Krylov.roots_quadratic-Union{Tuple{T}, Tuple{T, T, T}} where T&lt;:AbstractFloat" href="#Krylov.roots_quadratic-Union{Tuple{T}, Tuple{T, T, T}} where T&lt;:AbstractFloat"><code>Krylov.roots_quadratic</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">roots = roots_quadratic(q₂, q₁, q₀; nitref)</code></pre><p>Find the real roots of the quadratic</p><pre><code class="nohighlight hljs">q(x) = q₂ x² + q₁ x + q₀,</code></pre><p>where q₂, q₁ and q₀ are real. Care is taken to avoid numerical cancellation. Optionally, <code>nitref</code> steps of iterative refinement may be performed to improve accuracy. By default, <code>nitref=1</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/Krylov.jl/blob/94caa506e3363ec846b33c79ccc8889d96055e85/src/krylov_utils.jl#L89-L99">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Krylov.sym_givens-Union{Tuple{T}, Tuple{Complex{T}, Complex{T}}} where T&lt;:AbstractFloat" href="#Krylov.sym_givens-Union{Tuple{T}, Tuple{Complex{T}, Complex{T}}} where T&lt;:AbstractFloat"><code>Krylov.sym_givens</code></a> — <span class="docstring-category">Method</span></header><section><div><p>Numerically stable symmetric Givens reflection. Given <code>a</code> and <code>b</code> complexes, return <code>(c, s, ρ)</code> with c real and (s, ρ) complexes such that</p><pre><code class="nohighlight hljs">[ c   s ] [ a ] = [ ρ ]
[ s̅  -c ] [ b ] = [ 0 ].</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/Krylov.jl/blob/94caa506e3363ec846b33c79ccc8889d96055e85/src/krylov_utils.jl#L46-L53">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Krylov.sym_givens-Union{Tuple{T}, Tuple{T, T}} where T&lt;:AbstractFloat" href="#Krylov.sym_givens-Union{Tuple{T}, Tuple{T, T}} where T&lt;:AbstractFloat"><code>Krylov.sym_givens</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">(c, s, ρ) = sym_givens(a, b)</code></pre><p>Numerically stable symmetric Givens reflection. Given <code>a</code> and <code>b</code> reals, return <code>(c, s, ρ)</code> such that</p><pre><code class="nohighlight hljs">[ c  s ] [ a ] = [ ρ ]
[ s -c ] [ b ] = [ 0 ].</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/Krylov.jl/blob/94caa506e3363ec846b33c79ccc8889d96055e85/src/krylov_utils.jl#L1-L9">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Krylov.symmlq!-Union{Tuple{S}, Tuple{T}, Tuple{SymmlqSolver{T, S}, Any, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}" href="#Krylov.symmlq!-Union{Tuple{S}, Tuple{T}, Tuple{SymmlqSolver{T, S}, Any, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}"><code>Krylov.symmlq!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">solver = symmlq!(solver::SymmlqSolver, args...; kwargs...)</code></pre><p>where <code>args</code> and <code>kwargs</code> are arguments and keyword arguments of <a href="../solvers/sid/#Krylov.symmlq"><code>symmlq</code></a>.</p><p>See <a href="../api/#Krylov.SymmlqSolver"><code>SymmlqSolver</code></a> for more details about the <code>solver</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/Krylov.jl/blob/94caa506e3363ec846b33c79ccc8889d96055e85/src/symmlq.jl#L45-L51">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Krylov.symmlq-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T&lt;:AbstractFloat" href="#Krylov.symmlq-Union{Tuple{T}, Tuple{Any, AbstractVector{T}}} where T&lt;:AbstractFloat"><code>Krylov.symmlq</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">(x, stats) = symmlq(A, b::AbstractVector{T};
                    M=I, λ::T=zero(T), transfer_to_cg::Bool=true,
                    λest::T=zero(T), atol::T=√eps(T), rtol::T=√eps(T),
                    etol::T=√eps(T), window::Int=0, itmax::Int=0,
                    conlim::T=1/√eps(T), restart::Bool=false,
                    verbose::Int=0, history::Bool=false) where T &lt;: AbstractFloat</code></pre><p>Solve the shifted linear system</p><pre><code class="nohighlight hljs">(A + λI) x = b</code></pre><p>using the SYMMLQ method, where λ is a shift parameter, and A is square and symmetric.</p><p>SYMMLQ produces monotonic errors ‖x*-x‖₂.</p><p>A preconditioner M may be provided in the form of a linear operator and is assumed to be symmetric and positive definite.</p><p><strong>Reference</strong></p><ul><li>C. C. Paige and M. A. Saunders, <a href="https://doi.org/10.1137/0712047"><em>Solution of Sparse Indefinite Systems of Linear Equations</em></a>, SIAM Journal on Numerical Analysis, 12(4), pp. 617–629, 1975.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/Krylov.jl/blob/94caa506e3363ec846b33c79ccc8889d96055e85/src/symmlq.jl#L15-L38">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Krylov.to_boundary-Union{Tuple{T}, Tuple{Vector{T}, Vector{T}, T}} where T&lt;:Number" href="#Krylov.to_boundary-Union{Tuple{T}, Tuple{Vector{T}, Vector{T}, T}} where T&lt;:Number"><code>Krylov.to_boundary</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">roots = to_boundary(x, d, radius; flip, xNorm2, dNorm2)</code></pre><p>Given a trust-region radius <code>radius</code>, a vector <code>x</code> lying inside the trust-region and a direction <code>d</code>, return <code>σ1</code> and <code>σ2</code> such that</p><pre><code class="nohighlight hljs">‖x + σi d‖ = radius, i = 1, 2</code></pre><p>in the Euclidean norm. If known, ‖x‖² may be supplied in <code>xNorm2</code>.</p><p>If <code>flip</code> is set to <code>true</code>, <code>σ1</code> and <code>σ2</code> are computed such that</p><pre><code class="nohighlight hljs">‖x - σi d‖ = radius, i = 1, 2.</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/Krylov.jl/blob/94caa506e3363ec846b33c79ccc8889d96055e85/src/krylov_utils.jl#L140-L153">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Krylov.tricg!-Union{Tuple{S}, Tuple{T}, Tuple{TricgSolver{T, S}, Any, AbstractVector{T}, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}" href="#Krylov.tricg!-Union{Tuple{S}, Tuple{T}, Tuple{TricgSolver{T, S}, Any, AbstractVector{T}, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}"><code>Krylov.tricg!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">solver = tricg!(solver::TricgSolver, args...; kwargs...)</code></pre><p>where <code>args</code> and <code>kwargs</code> are arguments and keyword arguments of <a href="#Krylov.tricg"><code>tricg</code></a>.</p><p>See <a href="../../api/#Krylov.TricgSolver"><code>TricgSolver</code></a> for more details about the <code>solver</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/Krylov.jl/blob/94caa506e3363ec846b33c79ccc8889d96055e85/src/tricg.jl#L62-L68">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Krylov.tricg-Union{Tuple{T}, Tuple{Any, AbstractVector{T}, AbstractVector{T}}} where T&lt;:AbstractFloat" href="#Krylov.tricg-Union{Tuple{T}, Tuple{Any, AbstractVector{T}, AbstractVector{T}}} where T&lt;:AbstractFloat"><code>Krylov.tricg</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">(x, y, stats) = tricg(A, b::AbstractVector{T}, c::AbstractVector{T};
                      M=I, N=I, atol::T=√eps(T), rtol::T=√eps(T),
                      spd::Bool=false, snd::Bool=false, flip::Bool=false,
                      τ::T=one(T), ν::T=-one(T), itmax::Int=0, verbose::Int=0,
                      restart::Bool=false, history::Bool=false) where T &lt;: AbstractFloat</code></pre><p>TriCG solves the symmetric linear system</p><pre><code class="nohighlight hljs">[ τE    A ] [ x ] = [ b ]
[  Aᵀ  νF ] [ y ]   [ c ],</code></pre><p>where τ and ν are real numbers, E = M⁻¹ ≻ 0 and F = N⁻¹ ≻ 0. <code>b</code> and <code>c</code> must both be nonzero. TriCG could breakdown if <code>τ = 0</code> or <code>ν = 0</code>. It&#39;s recommended to use TriMR in these cases.</p><p>By default, TriCG solves symmetric and quasi-definite linear systems with τ = 1 and ν = -1. If <code>flip = true</code>, TriCG solves another known variant of SQD systems where τ = -1 and ν = 1. If <code>spd = true</code>, τ = ν = 1 and the associated symmetric and positive definite linear system is solved. If <code>snd = true</code>, τ = ν = -1 and the associated symmetric and negative definite linear system is solved. <code>τ</code> and <code>ν</code> are also keyword arguments that can be directly modified for more specific problems.</p><p>TriCG is based on the preconditioned orthogonal tridiagonalization process and its relation with the preconditioned block-Lanczos process.</p><pre><code class="nohighlight hljs">[ M   0 ]
[ 0   N ]</code></pre><p>indicates the weighted norm in which residuals are measured. It&#39;s the Euclidean norm when <code>M</code> and <code>N</code> are identity operators.</p><p>TriCG stops when <code>itmax</code> iterations are reached or when <code>‖rₖ‖ ≤ atol + ‖r₀‖ * rtol</code>. <code>atol</code> is an absolute tolerance and <code>rtol</code> is a relative tolerance.</p><p>Additional details can be displayed if verbose mode is enabled (verbose &gt; 0). Information will be displayed every <code>verbose</code> iterations.</p><p><strong>Reference</strong></p><ul><li>A. Montoison and D. Orban, <a href="https://doi.org/10.1137/20M1363030"><em>TriCG and TriMR: Two Iterative Methods for Symmetric Quasi-Definite Systems</em></a>, SIAM Journal on Scientific Computing, 43(4), pp. 2502–2525, 2021.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/Krylov.jl/blob/94caa506e3363ec846b33c79ccc8889d96055e85/src/tricg.jl#L14-L55">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Krylov.trilqr!-Union{Tuple{S}, Tuple{T}, Tuple{TrilqrSolver{T, S}, Any, AbstractVector{T}, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}" href="#Krylov.trilqr!-Union{Tuple{S}, Tuple{T}, Tuple{TrilqrSolver{T, S}, Any, AbstractVector{T}, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}"><code>Krylov.trilqr!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">solver = trilqr!(solver::TrilqrSolver, args...; kwargs...)</code></pre><p>where <code>args</code> and <code>kwargs</code> are arguments and keyword arguments of <a href="#Krylov.trilqr"><code>trilqr</code></a>.</p><p>See <a href="../../api/#Krylov.TrilqrSolver"><code>TrilqrSolver</code></a> for more details about the <code>solver</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/Krylov.jl/blob/94caa506e3363ec846b33c79ccc8889d96055e85/src/trilqr.jl#L41-L47">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Krylov.trilqr-Union{Tuple{T}, Tuple{Any, AbstractVector{T}, AbstractVector{T}}} where T&lt;:AbstractFloat" href="#Krylov.trilqr-Union{Tuple{T}, Tuple{Any, AbstractVector{T}, AbstractVector{T}}} where T&lt;:AbstractFloat"><code>Krylov.trilqr</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">(x, y, stats) = trilqr(A, b::AbstractVector{T}, c::AbstractVector{T};
                       atol::T=√eps(T), rtol::T=√eps(T), transfer_to_usymcg::Bool=true,
                       itmax::Int=0, verbose::Int=0, history::Bool=false) where T &lt;: AbstractFloat</code></pre><p>Combine USYMLQ and USYMQR to solve adjoint systems.</p><pre><code class="nohighlight hljs">[0  A] [y] = [b]
[Aᵀ 0] [x]   [c]</code></pre><p>USYMLQ is used for solving primal system <code>Ax = b</code>. USYMQR is used for solving dual system <code>Aᵀy = c</code>.</p><p>An option gives the possibility of transferring from the USYMLQ point to the USYMCG point, when it exists. The transfer is based on the residual norm.</p><p><strong>Reference</strong></p><ul><li>A. Montoison and D. Orban, <a href="https://doi.org/10.1137/19M1290991"><em>BiLQ: An Iterative Method for Nonsymmetric Linear Systems with a Quasi-Minimum Error Property</em></a>, SIAM Journal on Matrix Analysis and Applications, 41(3), pp. 1145–1166, 2020.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/Krylov.jl/blob/94caa506e3363ec846b33c79ccc8889d96055e85/src/trilqr.jl#L15-L34">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Krylov.trimr!-Union{Tuple{S}, Tuple{T}, Tuple{TrimrSolver{T, S}, Any, AbstractVector{T}, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}" href="#Krylov.trimr!-Union{Tuple{S}, Tuple{T}, Tuple{TrimrSolver{T, S}, Any, AbstractVector{T}, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}"><code>Krylov.trimr!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">solver = trimr!(solver::TrimrSolver, args...; kwargs...)</code></pre><p>where <code>args</code> and <code>kwargs</code> are arguments and keyword arguments of <a href="#Krylov.trimr"><code>trimr</code></a>.</p><p>See <a href="../../api/#Krylov.TrimrSolver"><code>TrimrSolver</code></a> for more details about the <code>solver</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/Krylov.jl/blob/94caa506e3363ec846b33c79ccc8889d96055e85/src/trimr.jl#L62-L68">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Krylov.trimr-Union{Tuple{T}, Tuple{Any, AbstractVector{T}, AbstractVector{T}}} where T&lt;:AbstractFloat" href="#Krylov.trimr-Union{Tuple{T}, Tuple{Any, AbstractVector{T}, AbstractVector{T}}} where T&lt;:AbstractFloat"><code>Krylov.trimr</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">(x, y, stats) = trimr(A, b::AbstractVector{T}, c::AbstractVector{T};
                      M=I, N=I, atol::T=√eps(T), rtol::T=√eps(T),
                      spd::Bool=false, snd::Bool=false, flip::Bool=false, sp::Bool=false,
                      τ::T=one(T), ν::T=-one(T), itmax::Int=0, verbose::Int=0,
                      restart::Bool=false, history::Bool=false) where T &lt;: AbstractFloat</code></pre><p>TriMR solves the symmetric linear system</p><pre><code class="nohighlight hljs">[ τE    A ] [ x ] = [ b ]
[  Aᵀ  νF ] [ y ]   [ c ],</code></pre><p>where τ and ν are real numbers, E = M⁻¹ ≻ 0, F = N⁻¹ ≻ 0. <code>b</code> and <code>c</code> must both be nonzero. TriMR handles saddle-point systems (<code>τ = 0</code> or <code>ν = 0</code>) and adjoint systems (<code>τ = 0</code> and <code>ν = 0</code>) without any risk of breakdown.</p><p>By default, TriMR solves symmetric and quasi-definite linear systems with τ = 1 and ν = -1. If <code>flip = true</code>, TriMR solves another known variant of SQD systems where τ = -1 and ν = 1. If <code>spd = true</code>, τ = ν = 1 and the associated symmetric and positive definite linear system is solved. If <code>snd = true</code>, τ = ν = -1 and the associated symmetric and negative definite linear system is solved. If <code>sp = true</code>, τ = 1, ν = 0 and the associated saddle-point linear system is solved. <code>τ</code> and <code>ν</code> are also keyword arguments that can be directly modified for more specific problems.</p><p>TriMR is based on the preconditioned orthogonal tridiagonalization process and its relation with the preconditioned block-Lanczos process.</p><pre><code class="nohighlight hljs">[ M   0 ]
[ 0   N ]</code></pre><p>indicates the weighted norm in which residuals are measured. It&#39;s the Euclidean norm when <code>M</code> and <code>N</code> are identity operators.</p><p>TriMR stops when <code>itmax</code> iterations are reached or when <code>‖rₖ‖ ≤ atol + ‖r₀‖ * rtol</code>. <code>atol</code> is an absolute tolerance and <code>rtol</code> is a relative tolerance.</p><p>Additional details can be displayed if verbose mode is enabled (verbose &gt; 0). Information will be displayed every <code>verbose</code> iterations.</p><p><strong>Reference</strong></p><ul><li>A. Montoison and D. Orban, <a href="https://doi.org/10.1137/20M1363030"><em>TriCG and TriMR: Two Iterative Methods for Symmetric Quasi-Definite Systems</em></a>, SIAM Journal on Scientific Computing, 43(4), pp. 2502–2525, 2021.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/Krylov.jl/blob/94caa506e3363ec846b33c79ccc8889d96055e85/src/trimr.jl#L14-L55">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Krylov.usymlq!-Union{Tuple{S}, Tuple{T}, Tuple{UsymlqSolver{T, S}, Any, AbstractVector{T}, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}" href="#Krylov.usymlq!-Union{Tuple{S}, Tuple{T}, Tuple{UsymlqSolver{T, S}, Any, AbstractVector{T}, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}"><code>Krylov.usymlq!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">solver = usymlq!(solver::UsymlqSolver, args...; kwargs...)</code></pre><p>where <code>args</code> and <code>kwargs</code> are arguments and keyword arguments of <a href="../solvers/unsymmetric/#Krylov.usymlq"><code>usymlq</code></a>.</p><p>See <a href="../api/#Krylov.UsymlqSolver"><code>UsymlqSolver</code></a> for more details about the <code>solver</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/Krylov.jl/blob/94caa506e3363ec846b33c79ccc8889d96055e85/src/usymlq.jl#L52-L58">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Krylov.usymlq-Union{Tuple{T}, Tuple{Any, AbstractVector{T}, AbstractVector{T}}} where T&lt;:AbstractFloat" href="#Krylov.usymlq-Union{Tuple{T}, Tuple{Any, AbstractVector{T}, AbstractVector{T}}} where T&lt;:AbstractFloat"><code>Krylov.usymlq</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">(x, stats) = usymlq(A, b::AbstractVector{T}, c::AbstractVector{T};
                    atol::T=√eps(T), rtol::T=√eps(T), transfer_to_usymcg::Bool=true,
                    itmax::Int=0, verbose::Int=0, history::Bool=false) where T &lt;: AbstractFloat</code></pre><p>Solve the linear system Ax = b using the USYMLQ method.</p><p>USYMLQ is based on the orthogonal tridiagonalization process and requires two initial nonzero vectors <code>b</code> and <code>c</code>. The vector <code>c</code> is only used to initialize the process and a default value can be <code>b</code> or <code>Aᵀb</code> depending on the shape of <code>A</code>. The error norm ‖x - x*‖ monotonously decreases in USYMLQ. It&#39;s considered as a generalization of SYMMLQ.</p><p>It can also be applied to under-determined and over-determined problems. In all cases, problems must be consistent.</p><p>An option gives the possibility of transferring to the USYMCG point, when it exists. The transfer is based on the residual norm.</p><p><strong>References</strong></p><ul><li>M. A. Saunders, H. D. Simon, and E. L. Yip, <a href="https://doi.org/10.1137/0725052"><em>Two Conjugate-Gradient-Type Methods for Unsymmetric Linear Equations</em></a>, SIAM Journal on Numerical Analysis, 25(4), pp. 927–940, 1988.</li><li>A. Buttari, D. Orban, D. Ruiz and D. Titley-Peloquin, <a href="https://doi.org/10.1137/18M1194900"><em>A tridiagonalization method for symmetric saddle-point and quasi-definite systems</em></a>, SIAM Journal on Scientific Computing, 41(5), pp. 409–432, 2019.</li><li>A. Montoison and D. Orban, <a href="https://doi.org/10.1137/19M1290991"><em>BiLQ: An Iterative Method for Nonsymmetric Linear Systems with a Quasi-Minimum Error Property</em></a>, SIAM Journal on Matrix Analysis and Applications, 41(3), pp. 1145–1166, 2020.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/Krylov.jl/blob/94caa506e3363ec846b33c79ccc8889d96055e85/src/usymlq.jl#L22-L45">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Krylov.usymqr!-Union{Tuple{S}, Tuple{T}, Tuple{UsymqrSolver{T, S}, Any, AbstractVector{T}, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}" href="#Krylov.usymqr!-Union{Tuple{S}, Tuple{T}, Tuple{UsymqrSolver{T, S}, Any, AbstractVector{T}, AbstractVector{T}}} where {T&lt;:AbstractFloat, S&lt;:DenseVector{T}}"><code>Krylov.usymqr!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">solver = usymqr!(solver::UsymqrSolver, args...; kwargs...)</code></pre><p>where <code>args</code> and <code>kwargs</code> are arguments and keyword arguments of <a href="../solvers/unsymmetric/#Krylov.usymqr"><code>usymqr</code></a>.</p><p>See <a href="../api/#Krylov.UsymqrSolver"><code>UsymqrSolver</code></a> for more details about the <code>solver</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/Krylov.jl/blob/94caa506e3363ec846b33c79ccc8889d96055e85/src/usymqr.jl#L49-L55">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Krylov.usymqr-Union{Tuple{T}, Tuple{Any, AbstractVector{T}, AbstractVector{T}}} where T&lt;:AbstractFloat" href="#Krylov.usymqr-Union{Tuple{T}, Tuple{Any, AbstractVector{T}, AbstractVector{T}}} where T&lt;:AbstractFloat"><code>Krylov.usymqr</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">(x, stats) = usymqr(A, b::AbstractVector{T}, c::AbstractVector{T};
                    atol::T=√eps(T), rtol::T=√eps(T),
                    itmax::Int=0, verbose::Int=0, history::Bool=false) where T &lt;: AbstractFloat</code></pre><p>Solve the linear system Ax = b using the USYMQR method.</p><p>USYMQR is based on the orthogonal tridiagonalization process and requires two initial nonzero vectors <code>b</code> and <code>c</code>. The vector <code>c</code> is only used to initialize the process and a default value can be <code>b</code> or <code>Aᵀb</code> depending on the shape of <code>A</code>. The residual norm ‖b - Ax‖ monotonously decreases in USYMQR. It&#39;s considered as a generalization of MINRES.</p><p>It can also be applied to under-determined and over-determined problems. USYMQR finds the minimum-norm solution if problems are inconsistent.</p><p><strong>References</strong></p><ul><li>M. A. Saunders, H. D. Simon, and E. L. Yip, <a href="https://doi.org/10.1137/0725052"><em>Two Conjugate-Gradient-Type Methods for Unsymmetric Linear Equations</em></a>, SIAM Journal on Numerical Analysis, 25(4), pp. 927–940, 1988.</li><li>A. Buttari, D. Orban, D. Ruiz and D. Titley-Peloquin, <a href="https://doi.org/10.1137/18M1194900"><em>A tridiagonalization method for symmetric saddle-point and quasi-definite systems</em></a>, SIAM Journal on Scientific Computing, 41(5), pp. 409–432, 2019.</li><li>A. Montoison and D. Orban, <a href="https://doi.org/10.1137/19M1290991"><em>BiLQ: An Iterative Method for Nonsymmetric Linear Systems with a Quasi-Minimum Error Property</em></a>, SIAM Journal on Matrix Analysis and Applications, 41(3), pp. 1145–1166, 2020.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/Krylov.jl/blob/94caa506e3363ec846b33c79ccc8889d96055e85/src/usymqr.jl#L22-L42">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="Krylov.vec2str-Union{Tuple{AbstractVector{T}}, Tuple{T}} where T&lt;:Union{Missing, AbstractFloat}" href="#Krylov.vec2str-Union{Tuple{AbstractVector{T}}, Tuple{T}} where T&lt;:Union{Missing, AbstractFloat}"><code>Krylov.vec2str</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">s = vec2str(x; ndisp)</code></pre><p>Display an array in the form</p><pre><code class="nohighlight hljs">[ -3.0e-01 -5.1e-01  1.9e-01 ... -2.3e-01 -4.4e-01  2.4e-01 ]</code></pre><p>with (ndisp - 1)/2 elements on each side.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/JuliaSmoothOptimizers/Krylov.jl/blob/94caa506e3363ec846b33c79ccc8889d96055e85/src/krylov_utils.jl#L260-L268">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../examples/">« Tutorial</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.12 on <span class="colophon-date" title="Tuesday 8 February 2022 03:53">Tuesday 8 February 2022</span>. Using Julia version 1.7.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
